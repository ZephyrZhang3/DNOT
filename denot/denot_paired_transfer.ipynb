{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Imports\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import gc\n",
                "import json\n",
                "import warnings\n",
                "from typing import List\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import numpy as np\n",
                "from diffusers import DDIMScheduler\n",
                "from tensorboardX import SummaryWriter\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import PngImagePlugin\n",
                "from tqdm import tqdm_notebook as tqdm\n",
                "from IPython.display import clear_output\n",
                "\n",
                "sys.path.append(\"..\")\n",
                "from src.enot import SDE, integrate\n",
                "from src.resnet2 import ResNet_D\n",
                "from src.cunet import CUNet\n",
                "\n",
                "from src.tools import (\n",
                "    set_random_seed,\n",
                "    unfreeze,\n",
                "    freeze,\n",
                "    weights_init_D,\n",
                "    fig2tensor,\n",
                "    get_linked_sde_pushed_loader_metrics,\n",
                "    get_linked_sde_pushed_loader_stats,\n",
                ")  # for wandb\n",
                "from src.fid_score import calculate_frechet_distance\n",
                "from src.plotters_paired import (\n",
                "    plot_linked_sde_pushed_images,\n",
                "    plot_linked_sde_pushed_random_paired_images,\n",
                ")\n",
                "\n",
                "from src.samplers import PairedLoaderSampler, get_paired_sampler\n",
                "\n",
                "LARGE_ENOUGH_NUMBER = 100\n",
                "PngImagePlugin.MAX_TEXT_CHUNK = LARGE_ENOUGH_NUMBER * (1024**2)\n",
                "\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "%matplotlib inline "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gc.collect()\n",
                "torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Config\n",
                "\n",
                "Dataset choosing in the first rows\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "SEED = 0x3060\n",
                "set_random_seed(SEED)\n",
                "\n",
                "# dataset choosing\n",
                "# face2comic\n",
                "# DATASET, DATASET_PATH, REVERSE = 'comic_faces_v1', '../datasets/face2comics_v1.0.0_by_Sxela', \"comic2face\", False\n",
                "# colored mask -> face\n",
                "# DATASET, DATASET_PATH, REVERSE = \"celeba_mask\", \"../datasets/CelebAMask-HQ\", \"colored_mask2face\", False\n",
                "# sketch -> photo\n",
                "DATASET, DATASET_PATH, MAP_NAME, REVERSE = (\n",
                "    \"FS2K\",\n",
                "    \"../datasets/FS2K/\",\n",
                "    \"sketch2photo\",\n",
                "    False,\n",
                ")\n",
                "\n",
                "IMG_SIZE = 256\n",
                "DATASET1_CHANNELS = 3\n",
                "DATASET2_CHANNELS = 3\n",
                "\n",
                "# the step number adding noise in diffusion process\n",
                "DIFFUSION_STEPS = 1000\n",
                "PIVOTAL_LIST = [50, 100, 200]\n",
                "\n",
                "# GPU choosing\n",
                "DEVICE_IDS = [0]\n",
                "assert torch.cuda.is_available()\n",
                "\n",
                "CONTINUE = [\n",
                "    0,\n",
                "    0,\n",
                "]  # first is for step, setting the value (checkpoints step + 1); last is for sdes, setting the value be (num of train-finished sde + 1).\n",
                "\n",
                "# All hyperparameters below is set to the values used for the experiments, which discribed in the article\n",
                "\n",
                "# training algorithm settings\n",
                "STRATEGY = \"Fix\"  # 'Fix' or 'Adapt'\n",
                "\n",
                "BATCH_SIZE = 2\n",
                "T_ITERS = 10\n",
                "MAX_STEPS = 5000 + 1  # 2501 for testing\n",
                "INTEGRAL_SCALE = 1 / (3 * IMG_SIZE * IMG_SIZE)\n",
                "EPSILON_SCHEDULER_LAST_ITER = 20000\n",
                "\n",
                "# optimizer settings\n",
                "D_LR, T_LR = 1e-4, 1e-4\n",
                "BETA_D, BETA_T = 0.9, 0.9\n",
                "T_GRADIENT_MAX_NORM = float(500)\n",
                "D_GRADIENT_MAX_NORM = float(500)\n",
                "\n",
                "# SDE network settings\n",
                "EPSILON = 0  # [0 , 1, 10]\n",
                "IMAGE_INPUT = True\n",
                "PREDICT_SHIFT = True\n",
                "N_STEPS = 5  # num of shifts time\n",
                "UNET_BASE_FACTOR = 128\n",
                "TIME_DIM = 128\n",
                "USE_POSITIONAL_ENCODING = True\n",
                "ONE_STEP_INIT_ITERS = 0\n",
                "USE_GRADIENT_CHECKPOINT = False\n",
                "N_LAST_STEPS_WITHOUT_NOISE = 1\n",
                "\n",
                "# plot settings\n",
                "GRAY_PLOTS = False\n",
                "STEPS_TO_SHOW = 10\n",
                "\n",
                "# log settings\n",
                "SMART_INTERVALS = False\n",
                "INTERVAL_SHRINK_START_TIME = 0.98\n",
                "TRACK_VAR_INTERVAL = 10\n",
                "PLOT_INTERVAL = 500\n",
                "CPKT_INTERVAL = 500\n",
                "\n",
                "FID_EPOCHS = 1\n",
                "\n",
                "EXP_NAME = f\"Ours_Paired_{DATASET}_{STRATEGY}_{SEED}\"\n",
                "OUTPUT_PATH = f\"../saved_models/{EXP_NAME}/\"\n",
                "\n",
                "if not os.path.exists(OUTPUT_PATH):\n",
                "    os.makedirs(OUTPUT_PATH)\n",
                "\n",
                "writer = SummaryWriter(f\"../logdir/{EXP_NAME}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "config = dict(\n",
                "    SEED=SEED,\n",
                "    DIFFUSION_STEPS=DIFFUSION_STEPS,\n",
                "    PIVOTAL_LIST=PIVOTAL_LIST,\n",
                "    DATASET=DATASET,\n",
                "    T_ITERS=T_ITERS,\n",
                "    D_LR=D_LR,\n",
                "    T_LR=T_LR,\n",
                "    STRATEGY=STRATEGY,\n",
                "    BATCH_SIZE=BATCH_SIZE,\n",
                "    UNET_BASE_FACTOR=UNET_BASE_FACTOR,\n",
                "    N_STEPS=N_STEPS,\n",
                "    EPSILON=EPSILON,\n",
                "    USE_POSITIONAL_ENCODING=USE_POSITIONAL_ENCODING,\n",
                "    TIME_DIM=TIME_DIM,\n",
                "    INTEGRAL_SCALE=INTEGRAL_SCALE,\n",
                "    ONE_STEP_INIT_ITERS=ONE_STEP_INIT_ITERS,\n",
                "    T_GRADIENT_MAX_NORM=T_GRADIENT_MAX_NORM,\n",
                "    D_GRADIENT_MAX_NORM=D_GRADIENT_MAX_NORM,\n",
                "    PREDICT_SHIFT=PREDICT_SHIFT,\n",
                "    SMART_INTERVALS=SMART_INTERVALS,\n",
                "    INTERVAL_SHRINK_START_TIME=INTERVAL_SHRINK_START_TIME,\n",
                "    USE_GRADIENT_CHECKPOINT=USE_GRADIENT_CHECKPOINT,\n",
                "    N_LAST_STEPS_WITHOUT_NOISE=N_LAST_STEPS_WITHOUT_NOISE,\n",
                "    TRACK_VAR_INTERVAL=TRACK_VAR_INTERVAL,\n",
                "    EPSILON_SCHEDULER_LAST_ITER=EPSILON_SCHEDULER_LAST_ITER,\n",
                "    FID_EPOCHS=FID_EPOCHS,\n",
                ")\n",
                "with open(os.path.join(OUTPUT_PATH, \"config.json\"), \"w\") as json_file:\n",
                "    json_str = json.dumps(config, indent=4)\n",
                "    json_file.write(json_str)\n",
                "\n",
                "log = dict(CONTINUE=CONTINUE)\n",
                "with open(os.path.join(OUTPUT_PATH, \"log.json\"), \"w\") as log_file:\n",
                "    log_str = json.dumps(log, indent=4)\n",
                "    log_file.write(log_str)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if not REVERSE:\n",
                "    filename = f\"../stats/{DATASET}_{MAP_NAME.split('2')[1]}_{IMG_SIZE}_test.json\"\n",
                "else:\n",
                "    filename = f\"../stats/{DATASET}_{MAP_NAME.split('2')[0]}_{IMG_SIZE}_test.json\"\n",
                "\n",
                "with open(filename, \"r\") as fp:\n",
                "    data_stats = json.load(fp)\n",
                "    mu_data, sigma_data = data_stats[\"mu\"], data_stats[\"sigma\"]\n",
                "del data_stats"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Initialize samplers\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "XY_sampler, XY_test_sampler = get_paired_sampler(\n",
                "    DATASET, DATASET_PATH, img_size=IMG_SIZE, batch_size=BATCH_SIZE, reverse=REVERSE\n",
                ")\n",
                "\n",
                "torch.cuda.empty_cache()\n",
                "gc.collect()\n",
                "clear_output()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### pivotal sampler\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "SCHEDULER = DDIMScheduler(num_train_timesteps=DIFFUSION_STEPS)\n",
                "\n",
                "\n",
                "def sample_all_pivotal(\n",
                "    XY_sampler: PairedLoaderSampler,\n",
                "    batch_size: int = 4,\n",
                ") -> List[torch.Tensor]:\n",
                "    pivotal_path = []\n",
                "\n",
                "    source, target = XY_sampler.sample(batch_size)\n",
                "\n",
                "    source_list = [source]\n",
                "    target_list = [target]\n",
                "    for i in range(min(DIFFUSION_STEPS, PIVOTAL_LIST[-1])):\n",
                "        source = SCHEDULER.add_noise(\n",
                "            source, torch.randn_like(source), torch.Tensor([i]).long()\n",
                "        )\n",
                "        target = SCHEDULER.add_noise(\n",
                "            target, torch.randn_like(target), torch.Tensor([i]).long()\n",
                "        )\n",
                "        if (i + 1) in PIVOTAL_LIST:\n",
                "            source_list.append(source)\n",
                "            target_list.append(target)\n",
                "\n",
                "    target_list.reverse()\n",
                "\n",
                "    pivotal_path.extend(source_list)\n",
                "    pivotal_path.extend(target_list[1:])  # just using source's last pivotal point\n",
                "    # pivotal_path.extend(target_list[:]) # 2 last pivotal points mapping\n",
                "\n",
                "    return pivotal_path\n",
                "\n",
                "\n",
                "# def sample_step_t_pivotal(\n",
                "#     XY_sampler: PairedLoaderSampler,\n",
                "#     batch_size: int = 4,\n",
                "#     pivotal_step: int = 0,\n",
                "# ):\n",
                "#     pivotal_path = sample_all_pivotal(XY_sampler, batch_size)\n",
                "#     pivotal_t, pivotal_t_next = (\n",
                "#         pivotal_path[pivotal_step],\n",
                "#         pivotal_path[pivotal_step + 1],\n",
                "#     )\n",
                "#     return pivotal_t, pivotal_t_next"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### mapping plotters\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_all_pivotal(\n",
                "    source: torch.Tensor,\n",
                "    target: torch.Tensor,\n",
                "    gray: bool = False,\n",
                ") -> list:\n",
                "    pivotal_path = []\n",
                "\n",
                "    source_list = [source]\n",
                "    target_list = [target]\n",
                "    for i in range(min(DIFFUSION_STEPS, PIVOTAL_LIST[-1])):\n",
                "        source = SCHEDULER.add_noise(\n",
                "            source, torch.randn_like(source), torch.Tensor([i]).long()\n",
                "        )\n",
                "        target = SCHEDULER.add_noise(\n",
                "            target, torch.randn_like(target), torch.Tensor([i]).long()\n",
                "        )\n",
                "        if (i + 1) in PIVOTAL_LIST:\n",
                "            source_list.append(source)\n",
                "            target_list.append(target)\n",
                "\n",
                "    target_list.reverse()\n",
                "\n",
                "    pivotal_path.extend(source_list)\n",
                "    pivotal_path.extend(target_list[1:])  # just using source's last pivotal point\n",
                "    # pivotal_path.extend(target_list[:]) # 2 last pivotal points mapping\n",
                "\n",
                "    imgs: np.ndarray = (\n",
                "        torch.stack(pivotal_path)\n",
                "        .to(\"cpu\")\n",
                "        .permute(0, 2, 3, 1)\n",
                "        .mul(0.5)\n",
                "        .add(0.5)\n",
                "        .numpy()\n",
                "        .clip(0, 1)\n",
                "    )\n",
                "    nrows, ncols = 1, len(pivotal_path)\n",
                "    fig = plt.figure(figsize=(1.5 * ncols, 1.5 * nrows), dpi=150)\n",
                "    for i, img in enumerate(imgs):\n",
                "        ax = fig.add_subplot(nrows, ncols, i + 1)\n",
                "        if gray:\n",
                "            ax.imshow(img, cmap=\"gray\")\n",
                "        else:\n",
                "            ax.imshow(img)\n",
                "        ax.get_yaxis().set_visible(False)\n",
                "        ax.get_xaxis().set_visible(False)\n",
                "        ax.set_yticks([])\n",
                "        ax.set_xticks([])\n",
                "        ax.set_title(f\"$X_{i}$\", fontsize=24)\n",
                "        if i == imgs.shape[0] - 1:\n",
                "            ax.set_title(\"Y\", fontsize=24)\n",
                "\n",
                "    torch.cuda.empty_cache()\n",
                "    gc.collect()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Models initialization\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "SDEs, BETA_NETs = [], []\n",
                "SDE_OPTs, BETA_NET_OPTs = [], []\n",
                "SDE_SCHEDULERs, BETA_NET_SCHEDULERs = [], []\n",
                "\n",
                "for i in range(len(PIVOTAL_LIST) * 2):\n",
                "    T = CUNet(\n",
                "        DATASET1_CHANNELS, DATASET2_CHANNELS, TIME_DIM, base_factor=UNET_BASE_FACTOR\n",
                "    ).cuda()\n",
                "\n",
                "    T = SDE(\n",
                "        shift_model=T,\n",
                "        epsilon=EPSILON,\n",
                "        n_steps=N_STEPS,\n",
                "        time_dim=TIME_DIM,\n",
                "        n_last_steps_without_noise=N_LAST_STEPS_WITHOUT_NOISE,\n",
                "        use_positional_encoding=USE_POSITIONAL_ENCODING,\n",
                "        use_gradient_checkpoint=USE_GRADIENT_CHECKPOINT,\n",
                "        predict_shift=PREDICT_SHIFT,\n",
                "        image_input=IMAGE_INPUT,\n",
                "    ).cuda()\n",
                "    SDEs.append(T)\n",
                "\n",
                "    D = ResNet_D(IMG_SIZE, nc=DATASET2_CHANNELS).cuda()\n",
                "    D.apply(weights_init_D)\n",
                "    BETA_NETs.append(D)\n",
                "\n",
                "    T_opt = torch.optim.Adam(\n",
                "        T.parameters(), lr=T_LR, weight_decay=1e-10, betas=(BETA_T, 0.999)\n",
                "    )\n",
                "    D_opt = torch.optim.Adam(\n",
                "        D.parameters(), lr=D_LR, weight_decay=1e-10, betas=(BETA_D, 0.999)\n",
                "    )\n",
                "    SDE_OPTs.append(T_opt)\n",
                "    BETA_NET_OPTs.append(D_opt)\n",
                "\n",
                "    T_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
                "        T_opt, milestones=[15000, 25000, 40000, 55000, 70000], gamma=0.5\n",
                "    )\n",
                "    D_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
                "        D_opt, milestones=[15000, 25000, 40000, 55000, 70000], gamma=0.5\n",
                "    )\n",
                "    SDE_SCHEDULERs.append(T_scheduler)\n",
                "    BETA_NET_SCHEDULERs.append(D_scheduler)\n",
                "\n",
                "\n",
                "if len(DEVICE_IDS) > 1 and CONTINUE[0] == 0 and CONTINUE[1] == 0:\n",
                "    for i in range(len(SDEs)):\n",
                "        SDEs[i] = nn.DataParallel(SDEs[i], device_ids=DEVICE_IDS)\n",
                "        BETA_NETs[i] = nn.DataParallel(BETA_NETs[i], device_ids=DEVICE_IDS)\n",
                "\n",
                "        print(f\"T{i} params:\", np.sum([np.prod(p.shape) for p in SDEs[i].parameters()]))\n",
                "        print(\n",
                "            f\"D{i} params:\",\n",
                "            np.sum([np.prod(p.shape) for p in BETA_NETs[i].parameters()]),\n",
                "        )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Load weights for continue training\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if CONTINUE[0] > 0 or CONTINUE[1] > 0:\n",
                "    print(\"Loading weights for continue training\")\n",
                "    if STRATEGY == \"Adapt\":\n",
                "        for i, (T, T_opt, T_scheduler, D, D_opt, D_scheduler) in enumerate(\n",
                "            zip(\n",
                "                SDEs,\n",
                "                SDE_OPTs,\n",
                "                SDE_SCHEDULERs,\n",
                "                BETA_NETs,\n",
                "                BETA_NET_OPTs,\n",
                "                BETA_NET_SCHEDULERs,\n",
                "            )\n",
                "        ):\n",
                "            if i > CONTINUE[1]:\n",
                "                if len(DEVICE_IDS) > 1:\n",
                "                    T = nn.DataParallel(T, device_ids=DEVICE_IDS)\n",
                "                    D = nn.DataParallel(D, device_ids=DEVICE_IDS)\n",
                "                continue\n",
                "            if i < CONTINUE[1]:\n",
                "                CKPT_DIR = os.path.join(OUTPUT_PATH, f\"iter{MAX_STEPS - 1}/\")\n",
                "            if i == CONTINUE[1]:\n",
                "                CKPT_DIR = os.path.join(OUTPUT_PATH, f\"iter{CONTINUE[0] - 1}/\")\n",
                "\n",
                "            T.load_state_dict(torch.load(os.path.join(CKPT_DIR, f\"T{i}_{SEED}.pt\")))\n",
                "            print(f\"{CKPT_DIR} T{i}_{SEED}.pt, loaded\")\n",
                "            T_opt.load_state_dict(\n",
                "                torch.load(os.path.join(CKPT_DIR, f\"T_opt{i}_{SEED}.pt\"))\n",
                "            )\n",
                "            print(f\"{CKPT_DIR} T_opt{i}_{SEED}.pt, loaded\")\n",
                "            T_scheduler[i].load_state_dict(\n",
                "                torch.load(os.path.join(CKPT_DIR, f\"T_scheduler{i}_{SEED}.pt\"))\n",
                "            )\n",
                "            print(f\"{CKPT_DIR} T_scheduler{i}_{SEED}.pt, loaded\")\n",
                "\n",
                "            D.load_state_dict(torch.load(os.path.join(CKPT_DIR, f\"D{i}_{SEED}.pt\")))\n",
                "            print(f\"{CKPT_DIR} D{i}_{SEED}.pt, loaded\")\n",
                "            D_opt.load_state_dict(\n",
                "                torch.load(os.path.join(CKPT_DIR, f\"D_opt{i}_{SEED}.pt\"))\n",
                "            )\n",
                "            print(f\"{CKPT_DIR} D_opt{i}_{SEED}.pt, loaded\")\n",
                "            D_scheduler.load_state_dict(\n",
                "                torch.load(os.path.join(CKPT_DIR, f\"D_scheduler{i}_{SEED}.pt\"))\n",
                "            )\n",
                "            print(f\"{CKPT_DIR} D_scheduler{i}_{SEED}.pt, loaded\")\n",
                "            if len(DEVICE_IDS) > 1:\n",
                "                T = nn.DataParallel(T, device_ids=DEVICE_IDS)\n",
                "                D = nn.DataParallel(D, device_ids=DEVICE_IDS)\n",
                "\n",
                "    if STRATEGY == \"Fix\":\n",
                "        CKPT_DIR = os.path.join(OUTPUT_PATH, f\"iter{CONTINUE[0] - 1}/\")\n",
                "        for i, (T, T_opt, T_scheduler, D, D_opt, D_scheduler) in enumerate(\n",
                "            zip(\n",
                "                SDEs,\n",
                "                SDE_OPTs,\n",
                "                SDE_SCHEDULERs,\n",
                "                BETA_NETs,\n",
                "                BETA_NET_OPTs,\n",
                "                BETA_NET_SCHEDULERs,\n",
                "            )\n",
                "        ):\n",
                "            T.load_state_dict(torch.load(os.path.join(CKPT_DIR, f\"T{i}_{SEED}.pt\")))\n",
                "            print(f\"{CKPT_DIR} T{i}_{SEED}.pt, loaded\")\n",
                "            T_opt.load_state_dict(\n",
                "                torch.load(os.path.join(CKPT_DIR, f\"T_opt{i}_{SEED}.pt\"))\n",
                "            )\n",
                "            print(f\"{CKPT_DIR} T_opt{i}_{SEED}.pt, loaded\")\n",
                "            T_scheduler[i].load_state_dict(\n",
                "                torch.load(os.path.join(CKPT_DIR, f\"T_scheduler{i}_{SEED}.pt\"))\n",
                "            )\n",
                "            print(f\"{CKPT_DIR} T_scheduler{i}_{SEED}.pt, loaded\")\n",
                "\n",
                "            D.load_state_dict(torch.load(os.path.join(CKPT_DIR, f\"D{i}_{SEED}.pt\")))\n",
                "            print(f\"{CKPT_DIR} D{i}_{SEED}.pt, loaded\")\n",
                "            D_opt.load_state_dict(\n",
                "                torch.load(os.path.join(CKPT_DIR, f\"D_opt{i}_{SEED}.pt\"))\n",
                "            )\n",
                "            print(f\"{CKPT_DIR} D_opt{i}_{SEED}.pt, loaded\")\n",
                "            D_scheduler.load_state_dict(\n",
                "                torch.load(os.path.join(CKPT_DIR, f\"D_scheduler{i}_{SEED}.pt\"))\n",
                "            )\n",
                "            print(f\"{CKPT_DIR} D_scheduler{i}_{SEED}.pt, loaded\")\n",
                "            if len(DEVICE_IDS) > 1:\n",
                "                T = nn.DataParallel(T, device_ids=DEVICE_IDS)\n",
                "                D = nn.DataParallel(D, device_ids=DEVICE_IDS)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# writer.add_graph(\n",
                "#     SDEs[0], torch.rand(BATCH_SIZE, DATASET1_CHANNELS, IMG_SIZE, IMG_SIZE).cuda()\n",
                "# )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Plots Test\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_fixed, Y_fixed = XY_sampler.sample(BATCH_SIZE)\n",
                "X_test_fixed, Y_test_fixed = XY_test_sampler.sample(BATCH_SIZE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_all_pivotal(X_test_fixed[0], Y_test_fixed[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plot_linked_sde_pushed_images(X_fixed, Y_fixed, SDEs, gray=GRAY_PLOTS)\n",
                "\n",
                "writer.add_image(\"paired images[linked sde]\", fig2tensor(fig))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plot_linked_sde_pushed_random_paired_images(\n",
                "    XY_sampler, SDEs, plot_n_samples=BATCH_SIZE, gray=GRAY_PLOTS\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Main training cycle and logging\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gc.collect()\n",
                "torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def epsilon_scheduler(step):\n",
                "    return min(EPSILON, EPSILON * (step / EPSILON_SCHEDULER_LAST_ITER))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Fix strategy\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if STRATEGY == \"Fix\":\n",
                "    progress_bar = tqdm(total=MAX_STEPS, initial=CONTINUE[0])\n",
                "\n",
                "    for step in range(MAX_STEPS):\n",
                "        if step < CONTINUE[0]:\n",
                "            continue\n",
                "\n",
                "        for T, D in zip(SDEs, BETA_NETs):\n",
                "            freeze(T)\n",
                "            freeze(D)\n",
                "\n",
                "        new_epsilon = epsilon_scheduler(step)\n",
                "        writer.add_scalar(\"Epsilon\", new_epsilon, step)\n",
                "        for _ in range(T_ITERS):\n",
                "            fixed_trajectory = sample_all_pivotal(XY_sampler, BATCH_SIZE)\n",
                "\n",
                "            for i, (T, T_opt, D) in enumerate(zip(SDEs, SDE_OPTs, BETA_NETs)):\n",
                "                freeze(D)\n",
                "                unfreeze(T)\n",
                "\n",
                "                if len(DEVICE_IDS) > 1:\n",
                "                    T.module.set_epsilon(new_epsilon)\n",
                "                else:\n",
                "                    T.set_epsilon(new_epsilon)\n",
                "\n",
                "                T_opt.zero_grad()\n",
                "\n",
                "                X0, X1 = fixed_trajectory[i], fixed_trajectory[i + 1]\n",
                "\n",
                "                trajectory, times, shifts = T(X0)\n",
                "                XN = trajectory[:, -1]\n",
                "\n",
                "                norm = torch.norm(shifts.flatten(start_dim=2), p=2, dim=-1) ** 2\n",
                "                integral = INTEGRAL_SCALE * integrate(norm, times[0])\n",
                "\n",
                "                T_loss = (integral + D(X1) - D(XN)).mean()\n",
                "                writer.add_scalar(f\"T_loss/T{i}\", T_loss.item(), step)\n",
                "                T_loss.backward()\n",
                "                T_gradient_norm = torch.nn.utils.clip_grad_norm_(\n",
                "                    T.parameters(), max_norm=T_GRADIENT_MAX_NORM\n",
                "                )\n",
                "                T_opt.step()\n",
                "                del trajectory, X0, X1, XN, times, shifts\n",
                "                gc.collect()\n",
                "                torch.cuda.empty_cache()\n",
                "            del fixed_trajectory\n",
                "            gc.collect()\n",
                "            torch.cuda.empty_cache()\n",
                "        for T_scheduler in SDE_SCHEDULERs:\n",
                "            T_scheduler.step()\n",
                "        # wandb.log({f\"T gradient norm\": T_gradient_norm.item()}, step=step)\n",
                "        # wandb.log({f\"Mean norm\": torch.sqrt(norm).mean().item()}, step=step)\n",
                "        # wandb.log({f\"T_loss\": T_loss.item()}, step=step)\n",
                "\n",
                "        for T, D in zip(SDEs, BETA_NETs):\n",
                "            freeze(T)\n",
                "            freeze(D)\n",
                "\n",
                "        fixed_trajectory = sample_all_pivotal(XY_sampler, BATCH_SIZE)\n",
                "        for i, (D, D_opt, D_scheduler, T) in enumerate(\n",
                "            zip(BETA_NETs, BETA_NET_OPTs, BETA_NET_SCHEDULERs, SDEs)\n",
                "        ):\n",
                "            freeze(T)\n",
                "            unfreeze(D)\n",
                "\n",
                "            D_opt.zero_grad()\n",
                "\n",
                "            X0, X1 = fixed_trajectory[i], fixed_trajectory[i + 1]\n",
                "            trajectory, times, shifts = T(X0)\n",
                "            XN = trajectory[:, -1]\n",
                "            norm = torch.norm(shifts.flatten(start_dim=2), p=2, dim=-1) ** 2\n",
                "            integral = INTEGRAL_SCALE * integrate(norm, times[0])\n",
                "\n",
                "            D_loss = (-integral - D(X1) + D(XN)).mean()\n",
                "            writer.add_scalar(f\"D_loss/D{i}\", D_loss.item(), step)\n",
                "            D_loss.backward()\n",
                "            D_gradient_norm = torch.nn.utils.clip_grad_norm_(\n",
                "                D.parameters(), max_norm=D_GRADIENT_MAX_NORM\n",
                "            )\n",
                "            D_opt.step()\n",
                "            D_scheduler.step()\n",
                "\n",
                "            del trajectory, X0, X1, XN, times, shifts\n",
                "            gc.collect()\n",
                "            torch.cuda.empty_cache()\n",
                "        del fixed_trajectory\n",
                "        gc.collect()\n",
                "        torch.cuda.empty_cache()\n",
                "        # wandb.log({f\"D gradient norm\": D_gradient_norm.item()}, step=step)\n",
                "        # wandb.log({f\"D_loss\": D_loss.item()}, step=step)\n",
                "        # wandb.log({f\"integral\": integral.mean().item()}, step=step)\n",
                "        # wandb.log({f\"D_X1\": D_X1.mean().item()}, step=step)\n",
                "        # wandb.log({f\"D_XN\": D_XN.mean().item()}, step=step)\n",
                "\n",
                "        CONTINUE[0] += 1\n",
                "        progress_bar.update(1)\n",
                "\n",
                "        if step % PLOT_INTERVAL == 0:\n",
                "            progress_bar.close()\n",
                "            clear_output(wait=True)\n",
                "            progress_bar = tqdm(total=MAX_STEPS, initial=CONTINUE[0])\n",
                "            print(\"Plotting\")\n",
                "\n",
                "            inference_SDEs = SDEs\n",
                "            for T in inference_SDEs:\n",
                "                T.eval()\n",
                "            print(\"Fixed Test Images\")\n",
                "            fig, axes = plot_linked_sde_pushed_images(\n",
                "                X_test_fixed, Y_test_fixed, inference_SDEs, gray=GRAY_PLOTS\n",
                "            )\n",
                "            writer.add_image(\"Fixed Test Images\", fig2tensor(fig), step)\n",
                "            plt.show(fig)\n",
                "            plt.close(fig)\n",
                "            print(\"Random Test Images\")\n",
                "            fig, axes = plot_linked_sde_pushed_random_paired_images(\n",
                "                XY_test_sampler,\n",
                "                inference_SDEs,\n",
                "                plot_n_samples=BATCH_SIZE,\n",
                "                gray=GRAY_PLOTS,\n",
                "            )\n",
                "            writer.add_image(\"Random Test Images\", fig2tensor(fig), step)\n",
                "            plt.show(fig)\n",
                "            plt.close(fig)\n",
                "\n",
                "        if step != 0 and step % CPKT_INTERVAL == 0:\n",
                "            CKPT_DIR = os.path.join(OUTPUT_PATH, f\"iter{step}/\")\n",
                "            os.makedirs(CKPT_DIR, exist_ok=True)\n",
                "            for i, (T, T_opt, T_scheduler, D, D_opt, D_scheduler) in enumerate(\n",
                "                zip(\n",
                "                    SDEs,\n",
                "                    SDE_OPTs,\n",
                "                    SDE_SCHEDULERs,\n",
                "                    BETA_NETs,\n",
                "                    BETA_NET_OPTs,\n",
                "                    BETA_NET_SCHEDULERs,\n",
                "                )\n",
                "            ):\n",
                "                if len(DEVICE_IDS) > 1:\n",
                "                    torch.save(\n",
                "                        T.module.state_dict(),\n",
                "                        os.path.join(CKPT_DIR, f\"T{i}_{SEED}.pt\"),\n",
                "                    )\n",
                "                    torch.save(\n",
                "                        D.module.state_dict(),\n",
                "                        os.path.join(CKPT_DIR, f\"D{i}_{SEED}.pt\"),\n",
                "                    )\n",
                "                else:\n",
                "                    torch.save(\n",
                "                        T.state_dict(), os.path.join(CKPT_DIR, f\"T{i}_{SEED}.pt\")\n",
                "                    )\n",
                "                    torch.save(\n",
                "                        D.state_dict(), os.path.join(CKPT_DIR, f\"D{i}_{SEED}.pt\")\n",
                "                    )\n",
                "\n",
                "                torch.save(\n",
                "                    D_opt.state_dict(),\n",
                "                    os.path.join(CKPT_DIR, f\"D_opt{i}_{SEED}.pt\"),\n",
                "                )\n",
                "                torch.save(\n",
                "                    T_opt.state_dict(),\n",
                "                    os.path.join(CKPT_DIR, f\"T_opt{i}_{SEED}.pt\"),\n",
                "                )\n",
                "                torch.save(\n",
                "                    D_scheduler.state_dict(),\n",
                "                    os.path.join(CKPT_DIR, f\"D_scheduler{i}_{SEED}.pt\"),\n",
                "                )\n",
                "                torch.save(\n",
                "                    T_scheduler.state_dict(),\n",
                "                    os.path.join(CKPT_DIR, f\"T_scheduler{i}_{SEED}.pt\"),\n",
                "                )\n",
                "            log[\"CONTINUE\"] = CONTINUE\n",
                "            with open(os.path.join(OUTPUT_PATH, \"log.json\"), \"w\") as log_file:\n",
                "                log_str = json.dumps(log, indent=4)\n",
                "                log_file.write(log_str)\n",
                "\n",
                "        if step % TRACK_VAR_INTERVAL == 0:\n",
                "            pass\n",
                "            # print(\"Computing FID\")\n",
                "            # mu, sigma = get_linked_sde_pushed_loader_stats(\n",
                "            #     SDEs,\n",
                "            #     XY_test_sampler.loader,\n",
                "            #     n_epochs=FID_EPOCHS,\n",
                "            #     batch_size=BATCH_SIZE,\n",
                "            #     verbose=True,\n",
                "            # )\n",
                "            # fid = calculate_frechet_distance(mu_data, sigma_data, mu, sigma)\n",
                "            # print(f\"FID={fid}\")\n",
                "            # writer.add_scalar(\"Metrics/FID\", fid, step)\n",
                "            # del mu, sigma\n",
                "\n",
                "            # print(\"Computing LPIPS(vgg) LPIPS(alex) L1 MSE\")\n",
                "            # metrics = get_linked_sde_pushed_loader_metrics(\n",
                "            #     SDEs,\n",
                "            #     XY_test_sampler.loader,\n",
                "            #     n_epochs=FID_EPOCHS,\n",
                "            #     batch_size=BATCH_SIZE,\n",
                "            #     verbose=True,\n",
                "            #     log_metrics=[\"mse\", \"l1\"]\n",
                "            # )\n",
                "            # print(f\"metrics={metrics}\")\n",
                "            # writer.add_scalar(\"Metrics/LPIPS(VGG)\", metrics[\"vgg\"], step)\n",
                "            # writer.add_scalar(\"Metrics/LPIPS(Alex)\", metrics[\"alex\"], step)\n",
                "            # writer.add_scalar(\"Metrics/L1\", metrics[\"l1\"], step)\n",
                "            # writer.add_scalar(\"Metrics/MSE\", metrics[\"mse\"], step)\n",
                "\n",
                "        gc.collect()\n",
                "        torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Adapt strategy\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if STRATEGY == \"Adapt\":\n",
                "    for i, (T, T_opt, T_scheduler, D, D_opt, D_scheduler) in enumerate(\n",
                "        zip(\n",
                "            SDEs,\n",
                "            SDE_OPTs,\n",
                "            SDE_SCHEDULERs,\n",
                "            BETA_NETs,\n",
                "            BETA_NET_OPTs,\n",
                "            BETA_NET_SCHEDULERs,\n",
                "        )\n",
                "    ):\n",
                "        if i < CONTINUE[1]:\n",
                "            continue\n",
                "        progress_bar = tqdm(\n",
                "            total=MAX_STEPS, initial=CONTINUE[0], desc=f\"{i + 1}/{len(SDEs)}:\"\n",
                "        )\n",
                "        for _T, _D in zip(SDEs, BETA_NETs):\n",
                "            freeze(_T)\n",
                "            freeze(_D)\n",
                "\n",
                "        for step in range(MAX_STEPS):\n",
                "            if step < CONTINUE[0]:\n",
                "                continue\n",
                "\n",
                "            for _ in range(T_ITERS):\n",
                "                freeze(D)\n",
                "                unfreeze(T)\n",
                "\n",
                "                new_epsilon = epsilon_scheduler(step)\n",
                "                if len(DEVICE_IDS) > 1:\n",
                "                    T.module.set_epsilon(new_epsilon)\n",
                "                else:\n",
                "                    T.set_epsilon(new_epsilon)\n",
                "                writer.add_scalar(f\"Epsilon/eps{i}\", new_epsilon, step)\n",
                "\n",
                "                # === sampler training data ===\n",
                "                fixed_trajectory = sample_all_pivotal(XY_sampler, BATCH_SIZE)\n",
                "                X0, X1 = fixed_trajectory[0], fixed_trajectory[i + 1]\n",
                "                with torch.no_grad():\n",
                "                    for _ in range(i):\n",
                "                        tmp_trajectory, _, _ = SDEs[i](X0)\n",
                "                        X0 = tmp_trajectory[:, -1]\n",
                "                X0 = X0.requires_grad_()\n",
                "                # === mapping and optimize ===\n",
                "                T_opt.zero_grad()\n",
                "\n",
                "                trajectory, times, shifts = T(X0)\n",
                "                XN = trajectory[:, -1]\n",
                "\n",
                "                norm = torch.norm(shifts.flatten(start_dim=2), p=2, dim=-1) ** 2\n",
                "                integral = INTEGRAL_SCALE * integrate(norm, times[0])\n",
                "                T_loss = (integral + D(X1) - D(XN)).mean()\n",
                "                writer.add_scalar(f\"T_loss/T{i}\", T_loss.item(), step)\n",
                "                T_loss.backward()\n",
                "                T_gradient_norm = torch.nn.utils.clip_grad_norm_(\n",
                "                    T.parameters(), max_norm=T_GRADIENT_MAX_NORM\n",
                "                )\n",
                "                T_opt.step()\n",
                "                del fixed_trajectory, trajectory, X0, X1, XN, times, shifts\n",
                "                gc.collect()\n",
                "                torch.cuda.empty_cache()\n",
                "            T_scheduler.step()\n",
                "            # wandb.log({f\"T gradient norm\": T_gradient_norm.item()}, step=step)\n",
                "            # wandb.log({f\"Mean norm\": torch.sqrt(norm).mean().item()}, step=step)\n",
                "            # wandb.log({f\"T_loss\": T_loss.item()}, step=step)\n",
                "\n",
                "            freeze(T)\n",
                "            unfreeze(D)\n",
                "            # === sampler training data ===\n",
                "            fixed_trajectory = sample_all_pivotal(XY_sampler, BATCH_SIZE)\n",
                "            X0, X1 = fixed_trajectory[0], fixed_trajectory[i + 1]\n",
                "            with torch.no_grad():\n",
                "                for _ in range(i):\n",
                "                    tmp_trajectory, _, _ = SDEs[i](X0)\n",
                "                    X0 = tmp_trajectory[:, -1]\n",
                "            X0 = X0.requires_grad_()\n",
                "            # === mapping and optimize ===\n",
                "            D_opt.zero_grad()\n",
                "            trajectory, times, shifts = T(X0)\n",
                "            XN = trajectory[:, -1]\n",
                "\n",
                "            norm = torch.norm(shifts.flatten(start_dim=2), p=2, dim=-1) ** 2\n",
                "            integral = INTEGRAL_SCALE * integrate(norm, times[0])\n",
                "            D_loss = (-integral - D(X1) + D(XN)).mean()\n",
                "            writer.add_scalar(f\"D_loss/D{i}\", D_loss.item(), step)\n",
                "            D_loss.backward()\n",
                "            D_gradient_norm = torch.nn.utils.clip_grad_norm_(\n",
                "                D.parameters(), max_norm=D_GRADIENT_MAX_NORM\n",
                "            )\n",
                "            D_opt.step()\n",
                "            D_scheduler.step()\n",
                "\n",
                "            del fixed_trajectory, trajectory, X0, X1, XN, times, shifts, D_loss\n",
                "            gc.collect()\n",
                "            torch.cuda.empty_cache()\n",
                "            # wandb.log({f\"D gradient norm\": D_gradient_norm.item()}, step=step)\n",
                "            # wandb.log({f\"D_loss\": D_loss.item()}, step=step)\n",
                "\n",
                "            # wandb.log({f\"integral\": integral.mean().item()}, step=step)\n",
                "            # wandb.log({f\"D_X1\": D_X1.mean().item()}, step=step)\n",
                "            # wandb.log({f\"D_XN\": D_XN.mean().item()}, step=step)\n",
                "\n",
                "            CONTINUE[0] += 1\n",
                "            progress_bar.update(1)\n",
                "\n",
                "            if step % PLOT_INTERVAL == 0:\n",
                "                progress_bar.close()\n",
                "                clear_output(wait=True)\n",
                "                progress_bar = tqdm(\n",
                "                    total=MAX_STEPS, initial=CONTINUE[0], desc=f\"{i + 1}/{len(SDEs)}:\"\n",
                "                )\n",
                "                print(\"Plotting\")\n",
                "\n",
                "                inference_SDEs = SDEs\n",
                "                for T in inference_SDEs:\n",
                "                    T.eval()\n",
                "\n",
                "                print(\"Fixed Test Images\")\n",
                "                fig, axes = plot_linked_sde_pushed_images(\n",
                "                    X_test_fixed, Y_test_fixed, inference_SDEs, gray=GRAY_PLOTS\n",
                "                )\n",
                "                writer.add_image(\"Fixed Test Images\", fig2tensor(fig), step)\n",
                "                plt.show(fig)\n",
                "                plt.close(fig)\n",
                "\n",
                "                print(\"Random Test Images\")\n",
                "                fig, axes = plot_linked_sde_pushed_random_paired_images(\n",
                "                    XY_test_sampler,\n",
                "                    inference_SDEs,\n",
                "                    plot_n_samples=BATCH_SIZE,\n",
                "                    gray=GRAY_PLOTS,\n",
                "                )\n",
                "                writer.add_image(\"Random Test Images\", fig2tensor(fig), step)\n",
                "                plt.show(fig)\n",
                "                plt.close(fig)\n",
                "\n",
                "            if step != 0 and step % CPKT_INTERVAL == 0:\n",
                "                CKPT_DIR = os.path.join(OUTPUT_PATH, f\"iter{step}/\")\n",
                "                os.makedirs(CKPT_DIR, exist_ok=True)\n",
                "                if len(DEVICE_IDS) > 1:\n",
                "                    torch.save(\n",
                "                        T.module.state_dict(),\n",
                "                        os.path.join(CKPT_DIR, f\"T{i}_{SEED}.pt\"),\n",
                "                    )\n",
                "                    torch.save(\n",
                "                        D.module.state_dict(),\n",
                "                        os.path.join(CKPT_DIR, f\"D{i}_{SEED}.pt\"),\n",
                "                    )\n",
                "                else:\n",
                "                    torch.save(\n",
                "                        T.state_dict(), os.path.join(CKPT_DIR, f\"T{i}_{SEED}.pt\")\n",
                "                    )\n",
                "                    torch.save(\n",
                "                        D.state_dict(), os.path.join(CKPT_DIR, f\"D{i}_{SEED}.pt\")\n",
                "                    )\n",
                "\n",
                "                torch.save(\n",
                "                    D_opt.state_dict(),\n",
                "                    os.path.join(CKPT_DIR, f\"D_opt{i}_{SEED}.pt\"),\n",
                "                )\n",
                "                torch.save(\n",
                "                    T_opt.state_dict(),\n",
                "                    os.path.join(CKPT_DIR, f\"T_opt{i}_{SEED}.pt\"),\n",
                "                )\n",
                "                torch.save(\n",
                "                    D_scheduler.state_dict(),\n",
                "                    os.path.join(CKPT_DIR, f\"D_scheduler{i}_{SEED}.pt\"),\n",
                "                )\n",
                "                torch.save(\n",
                "                    T_scheduler.state_dict(),\n",
                "                    os.path.join(CKPT_DIR, f\"T_scheduler{i}_{SEED}.pt\"),\n",
                "                )\n",
                "                log = dict(CONTINUE=CONTINUE)\n",
                "                with open(os.path.join(OUTPUT_PATH, \"log.json\"), \"w\") as log_file:\n",
                "                    log_str = json.dumps(log, indent=4)\n",
                "                    log_file.write(log)\n",
                "\n",
                "            if i == len(SDEs) - 1 and step % TRACK_VAR_INTERVAL == 0:\n",
                "                print(\"Computing FID\")\n",
                "                mu, sigma = get_linked_sde_pushed_loader_stats(\n",
                "                    SDEs,\n",
                "                    XY_test_sampler.loader,\n",
                "                    n_epochs=FID_EPOCHS,\n",
                "                    batch_size=BATCH_SIZE,\n",
                "                    verbose=True,\n",
                "                )\n",
                "                fid = calculate_frechet_distance(mu_data, sigma_data, mu, sigma)\n",
                "                writer.add_scalar(f\"Metrics/FID{i}\", fid, step)\n",
                "                del mu, sigma\n",
                "\n",
                "                print(\"Computing LPIPS(vgg) LPIPS(alex) L1 MSE\")\n",
                "                metrics = get_linked_sde_pushed_loader_metrics(\n",
                "                    SDEs,\n",
                "                    XY_test_sampler.loader,\n",
                "                    n_epochs=FID_EPOCHS,\n",
                "                    batch_size=BATCH_SIZE,\n",
                "                    verbose=True,\n",
                "                    log_metrics=[\"mse\", \"l1\"],\n",
                "                )\n",
                "                print(f\"metrics={metrics}\")\n",
                "                writer.add_scalar(\"LPIPS(VGG)\", metrics[\"vgg\"], step)\n",
                "                writer.add_scalar(\"LPIPS(Alex)\", metrics[\"alex\"], step)\n",
                "                writer.add_scalar(\"L1\", metrics[\"l1\"], step)\n",
                "                writer.add_scalar(\"MSE\", metrics[\"mse\"], step)\n",
                "\n",
                "            gc.collect()\n",
                "            torch.cuda.empty_cache()\n",
                "        CONTINUE[0] = 0  # reset training steps to 0\n",
                "        CONTINUE[1] += 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Clear resources\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    writer.close()\n",
                "    progress_bar.close()\n",
                "except Exception as e:\n",
                "    print(e)"
            ]
        }
    ],
    "metadata": {
        "celltoolbar": "Tags",
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
