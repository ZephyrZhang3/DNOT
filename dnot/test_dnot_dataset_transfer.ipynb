{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Imports\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import gc\n",
                "import warnings\n",
                "from typing import List\n",
                "\n",
                "import torch\n",
                "import torchvision\n",
                "import numpy as np\n",
                "from diffusers import DDIMScheduler\n",
                "from torchvision.transforms import Compose, ToTensor, Resize, Normalize, Lambda\n",
                "\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import PngImagePlugin\n",
                "from IPython.display import clear_output\n",
                "\n",
                "sys.path.append(\"..\")\n",
                "from src.unet import UNet\n",
                "from src.fid_score import calculate_frechet_distance\n",
                "\n",
                "from src.tools import (\n",
                "    set_random_seed,\n",
                "    get_loader_stats,\n",
                "    get_linked_pushed_loader_stats,\n",
                "    get_linked_pushed_loader_metrics,\n",
                "    get_linked_pushed_loader_accuracy,\n",
                ")\n",
                "from src.plotters import (\n",
                "    plot_linked_pushed_images,\n",
                "    plot_linked_pushed_random_class_images,\n",
                ")\n",
                "\n",
                "from src.mnistm_utils import MNISTM\n",
                "from src.samplers import SubsetGuidedSampler, SubsetGuidedDataset, get_indicies_subset\n",
                "\n",
                "LARGE_ENOUGH_NUMBER = 100\n",
                "PngImagePlugin.MAX_TEXT_CHUNK = LARGE_ENOUGH_NUMBER * (1024**2)\n",
                "\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "%matplotlib inline "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "gc.collect()\n",
                "torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Config\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "SEED = 0x3060\n",
                "set_random_seed(SEED)\n",
                "\n",
                "# dataset choosing\n",
                "DATASET, DATASET_PATH = \"fmnist2mnist\", \"../datasets/\"\n",
                "# DATASET, DATASET_PATH = \"mnist2mnistm\", \"../datasets/\"\n",
                "# DATASET, DATASET_PATH = \"mnist2usps\", \"../datasets/\"\n",
                "# DATASET, DATASET_PATH = \"mnist2kmnist\", \"../datasets/\"\n",
                "\n",
                "IMG_SIZE = 32\n",
                "DATASET1_CHANNELS = 1\n",
                "DATASET2_CHANNELS = 1\n",
                "\n",
                "# the step number adding noise in diffusion process\n",
                "DIFFUSION_STEPS = 1000\n",
                "PIVOTAL_LIST = [20, 50, 100]  # [0, 100] for testing,  [0, 20, 50, 100]\n",
                "\n",
                "# GPU choosing\n",
                "DEVICE_IDS = [0]\n",
                "assert torch.cuda.is_available()\n",
                "torch.cuda.set_device(f\"cuda:{DEVICE_IDS[0]}\")\n",
                "\n",
                "CONTINUE = [0, 0]  # first is for step, last is for sdes\n",
                "\n",
                "# All hyperparameters below is set to the values used for the experiments, which discribed in the article\n",
                "\n",
                "# training algorithm settings\n",
                "STRATEGY = \"Adapt\"  # 'Fix' or 'Adapt'\n",
                "# model settings\n",
                "UNET_BASE_FACTOR = 48\n",
                "# data sample settings\n",
                "BATCH_SIZE = 32\n",
                "\n",
                "# plot settings\n",
                "GRAY_PLOTS = True\n",
                "PLOT_N_SAMPLES = 8\n",
                "\n",
                "FID_EPOCHS = 1\n",
                "\n",
                "EXP_NAME = f\"DNOT_Class_{DATASET}_{STRATEGY}_{SEED}\"\n",
                "LOAD_PATH = f\"../saved_models/{EXP_NAME}/\"\n",
                "\n",
                "if not os.path.exists(LOAD_PATH):\n",
                "    raise Exception(\"no such file or directory...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "source_subset = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
                "new_labels_source = {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}\n",
                "target_subset = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
                "new_labels_target = {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "ResNet(\n",
                            "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
                            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "  (relu): ReLU(inplace=True)\n",
                            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
                            "  (layer1): Sequential(\n",
                            "    (0): BasicBlock(\n",
                            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "      (relu): ReLU(inplace=True)\n",
                            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "    )\n",
                            "    (1): BasicBlock(\n",
                            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "      (relu): ReLU(inplace=True)\n",
                            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "    )\n",
                            "  )\n",
                            "  (layer2): Sequential(\n",
                            "    (0): BasicBlock(\n",
                            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
                            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "      (relu): ReLU(inplace=True)\n",
                            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "      (downsample): Sequential(\n",
                            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
                            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "      )\n",
                            "    )\n",
                            "    (1): BasicBlock(\n",
                            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "      (relu): ReLU(inplace=True)\n",
                            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "    )\n",
                            "  )\n",
                            "  (layer3): Sequential(\n",
                            "    (0): BasicBlock(\n",
                            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
                            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "      (relu): ReLU(inplace=True)\n",
                            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "      (downsample): Sequential(\n",
                            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
                            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "      )\n",
                            "    )\n",
                            "    (1): BasicBlock(\n",
                            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "      (relu): ReLU(inplace=True)\n",
                            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "    )\n",
                            "  )\n",
                            "  (layer4): Sequential(\n",
                            "    (0): BasicBlock(\n",
                            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
                            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "      (relu): ReLU(inplace=True)\n",
                            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "      (downsample): Sequential(\n",
                            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
                            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "      )\n",
                            "    )\n",
                            "    (1): BasicBlock(\n",
                            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "      (relu): ReLU(inplace=True)\n",
                            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
                            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                            "    )\n",
                            "  )\n",
                            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
                            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
                            ")"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "classifier = torchvision.models.resnet18()\n",
                "classifier.fc = torch.nn.Linear(in_features=512, out_features=10, bias=True)\n",
                "\n",
                "source_transform = Compose(\n",
                "    [\n",
                "        Resize((IMG_SIZE, IMG_SIZE)),\n",
                "        ToTensor(),\n",
                "        Normalize((0.5), (0.5)),\n",
                "    ]\n",
                ")\n",
                "target_transform = source_transform\n",
                "\n",
                "if DATASET == \"mnist2kmnist\":\n",
                "    source = torchvision.datasets.MNIST\n",
                "    target = torchvision.datasets.KMNIST\n",
                "    classifier.conv1 = torch.nn.Conv2d(\n",
                "        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
                "    )\n",
                "    classifier.load_state_dict(torch.load(\"../saved_models/classifiers/kmnist.pt\"))\n",
                "\n",
                "elif DATASET == \"fmnist2mnist\":\n",
                "    source = torchvision.datasets.FashionMNIST\n",
                "    target = torchvision.datasets.MNIST\n",
                "    classifier.conv1 = torch.nn.Conv2d(\n",
                "        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
                "    )\n",
                "    classifier.load_state_dict(torch.load(\"../saved_models/classifiers/mnist.pt\"))\n",
                "\n",
                "elif DATASET == \"mnist2usps\":\n",
                "    source = torchvision.datasets.MNIST\n",
                "    target = torchvision.datasets.USPS\n",
                "    classifier.conv1 = torch.nn.Conv2d(\n",
                "        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
                "    )\n",
                "    classifier.load_state_dict(torch.load(\"../saved_models/classifiers/usps.pt\"))\n",
                "\n",
                "elif DATASET == \"mnist2mnistm\":\n",
                "    DATASET1_CHANNELS = 3\n",
                "    DATASET2_CHANNELS = 3\n",
                "    classifier.load_state_dict(torch.load(\"../saved_models/classifiers/mnistm.pt\"))\n",
                "    GRAY_PLOTS = False\n",
                "    source = torchvision.datasets.MNIST\n",
                "    target = MNISTM\n",
                "    source_transform = Compose(\n",
                "        [\n",
                "            Resize((IMG_SIZE, IMG_SIZE)),\n",
                "            ToTensor(),\n",
                "            Normalize((0.5), (0.5)),\n",
                "            Lambda(lambda x: -x.repeat(3, 1, 1)),\n",
                "        ]\n",
                "    )\n",
                "    target_transform = Compose(\n",
                "        [Resize(IMG_SIZE), ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
                "    )\n",
                "\n",
                "classifier.cuda()\n",
                "classifier.eval()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Initialize samplers\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "source_test = source(\n",
                "    root=DATASET_PATH, train=False, download=True, transform=source_transform\n",
                ")\n",
                "source_subset_samples, source_labels, source_class_indicies = get_indicies_subset(\n",
                "    source_test,\n",
                "    new_labels=new_labels_source,\n",
                "    classes=len(source_subset),\n",
                "    subset_classes=source_subset,\n",
                ")\n",
                "source_test = torch.utils.data.TensorDataset(\n",
                "    torch.stack(source_subset_samples), torch.LongTensor(source_labels)\n",
                ")\n",
                "\n",
                "\n",
                "target_test = target(\n",
                "    root=DATASET_PATH, train=False, download=True, transform=target_transform\n",
                ")\n",
                "target_subset_samples, target_labels, target_class_indicies = get_indicies_subset(\n",
                "    target_test,\n",
                "    new_labels=new_labels_target,\n",
                "    classes=len(target_subset),\n",
                "    subset_classes=target_subset,\n",
                ")\n",
                "target_test = torch.utils.data.TensorDataset(\n",
                "    torch.stack(target_subset_samples), torch.LongTensor(target_labels)\n",
                ")\n",
                "\n",
                "full_set_test = SubsetGuidedDataset(\n",
                "    source_test,\n",
                "    target_test,\n",
                "    num_labeled=\"all\",\n",
                "    in_indicies=source_class_indicies,\n",
                "    out_indicies=target_class_indicies,\n",
                ")\n",
                "\n",
                "XY_test_sampler = SubsetGuidedSampler(full_set_test, subsetsize=1)\n",
                "\n",
                "# for accuracy\n",
                "X_test_loader = torch.utils.data.DataLoader(\n",
                "    source_test,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    shuffle=False,\n",
                "    num_workers=8,\n",
                "    # pin_memory=True,\n",
                ")\n",
                "Y_test_loader = torch.utils.data.DataLoader(\n",
                "    target_test,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    shuffle=False,\n",
                "    num_workers=8,\n",
                "    # pin_memory=True,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### pivotal sampler\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "SCHEDULER = DDIMScheduler(num_train_timesteps=DIFFUSION_STEPS)\n",
                "\n",
                "\n",
                "def sample_all_pivotal(\n",
                "    XY_sampler: SubsetGuidedSampler,\n",
                "    batch_size: int = 4,\n",
                ") -> List[torch.Tensor]:\n",
                "    pivotal_path = []\n",
                "\n",
                "    source, target = XY_sampler.sample(batch_size)\n",
                "    source_list = [source]\n",
                "    target_list = [target]\n",
                "    for i in range(min(DIFFUSION_STEPS, PIVOTAL_LIST[-1])):\n",
                "        source = SCHEDULER.add_noise(\n",
                "            source, torch.randn_like(source), torch.Tensor([i]).long()\n",
                "        )\n",
                "        target = SCHEDULER.add_noise(\n",
                "            target, torch.randn_like(target), torch.Tensor([i]).long()\n",
                "        )\n",
                "        if (i + 1) in PIVOTAL_LIST:\n",
                "            source_list.append(source)\n",
                "            target_list.append(target)\n",
                "\n",
                "    target_list.reverse()\n",
                "\n",
                "    pivotal_path.extend(source_list)\n",
                "    pivotal_path.extend(target_list[1:])  # just using source's last pivotal point\n",
                "    # pivotal_path.extend(target_list[:]) # 2 last pivotal points mapping\n",
                "\n",
                "    return pivotal_path\n",
                "\n",
                "\n",
                "# def sample_step_t_pivotal(\n",
                "#     XY_sampler: PairedLoaderSampler,\n",
                "#     batch_size: int = 4,\n",
                "#     pivotal_step: int = 0,\n",
                "# ):\n",
                "#     pivotal_path = sample_all_pivotal(XY_sampler, batch_size)\n",
                "#     pivotal_t, pivotal_t_next = (\n",
                "#         pivotal_path[pivotal_step],\n",
                "#         pivotal_path[pivotal_step + 1],\n",
                "#     )\n",
                "#     return pivotal_t, pivotal_t_next"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### mapping plotters\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_all_pivotal(\n",
                "    source: torch.Tensor,\n",
                "    target: torch.Tensor,\n",
                "    gray: bool = False,\n",
                "):\n",
                "    pivotal_path = []\n",
                "\n",
                "    source_list = [source]\n",
                "    target_list = [target]\n",
                "    for i in range(min(DIFFUSION_STEPS, PIVOTAL_LIST[-1])):\n",
                "        source = SCHEDULER.add_noise(\n",
                "            source, torch.randn_like(source), torch.Tensor([i]).long()\n",
                "        )\n",
                "        target = SCHEDULER.add_noise(\n",
                "            target, torch.randn_like(target), torch.Tensor([i]).long()\n",
                "        )\n",
                "        if (i + 1) in PIVOTAL_LIST:\n",
                "            source_list.append(source)\n",
                "            target_list.append(target)\n",
                "\n",
                "    target_list.reverse()\n",
                "\n",
                "    pivotal_path.extend(source_list)\n",
                "    pivotal_path.extend(target_list[1:])  # just using source's last pivotal point\n",
                "    # pivotal_path.extend(target_list[:]) # 2 last pivotal points mapping\n",
                "\n",
                "    imgs: np.ndarray = (\n",
                "        torch.stack(pivotal_path)\n",
                "        .to(\"cpu\")\n",
                "        .permute(0, 2, 3, 1)\n",
                "        .mul(0.5)\n",
                "        .add(0.5)\n",
                "        .numpy()\n",
                "        .clip(0, 1)\n",
                "    )\n",
                "    nrows, ncols = 1, len(pivotal_path)\n",
                "    fig = plt.figure(figsize=(1.5 * ncols, 1.5 * nrows), dpi=150)\n",
                "    for i, img in enumerate(imgs):\n",
                "        ax = fig.add_subplot(nrows, ncols, i + 1)\n",
                "        if gray:\n",
                "            ax.imshow(img, cmap=\"gray\")\n",
                "        else:\n",
                "            ax.imshow(img)\n",
                "        ax.get_yaxis().set_visible(False)\n",
                "        ax.get_xaxis().set_visible(False)\n",
                "        ax.set_yticks([])\n",
                "        ax.set_xticks([])\n",
                "        ax.set_title(f\"$X_{i}$\", fontsize=24)\n",
                "        if i == imgs.shape[0] - 1:\n",
                "            ax.set_title(\"Y\", fontsize=24)\n",
                "\n",
                "    torch.cuda.empty_cache()\n",
                "    gc.collect()\n",
                "\n",
                "    return fig, fig.axes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Testing\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### init models\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "Ts = []\n",
                "\n",
                "for i in range(len(PIVOTAL_LIST) * 2):\n",
                "    T = UNet(DATASET1_CHANNELS, DATASET2_CHANNELS, base_factor=UNET_BASE_FACTOR).cuda()\n",
                "    Ts.append(T)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Load weights for testing\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Loading weights\")\n",
                "\n",
                "CKPT_DIR = os.path.join(LOAD_PATH, f\"iter{2000}\")  # user setting\n",
                "for i, T in enumerate(Ts):\n",
                "    w_path = os.path.join(CKPT_DIR, f\"T{i}_{SEED}.pt\")\n",
                "    T.load_state_dict(torch.load(w_path))\n",
                "    print(f\"{w_path}, loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Plots Test\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_fixed, Y_fixed = XY_test_sampler.sample(10)\n",
                "X_fixed, Y_fixed = X_fixed.flatten(0, 1), Y_fixed.flatten(0, 1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plot_all_pivotal(X_fixed[0], Y_fixed[0], gray=GRAY_PLOTS)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plot_linked_pushed_images(X_fixed, Y_fixed, Ts, gray=GRAY_PLOTS)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plot_linked_pushed_random_class_images(\n",
                "    XY_test_sampler, Ts, plot_n_samples=PLOT_N_SAMPLES, gray=GRAY_PLOTS\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### main test\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gc.collect()\n",
                "torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "clear_output(wait=True)\n",
                "print(\"Plotting\")\n",
                "\n",
                "inference_Ts = Ts\n",
                "for T in inference_Ts:\n",
                "    T.eval()\n",
                "print(\"Fix Test Images\")\n",
                "fig, axes = plot_linked_pushed_images(X_fixed, Y_fixed, inference_Ts, gray=GRAY_PLOTS)\n",
                "plt.show(fig)\n",
                "plt.close(fig)\n",
                "print(\"Random Test Images\")\n",
                "fig, axes = plot_linked_pushed_random_class_images(\n",
                "    XY_test_sampler,\n",
                "    inference_Ts,\n",
                "    plot_n_samples=PLOT_N_SAMPLES,\n",
                "    gray=GRAY_PLOTS,\n",
                ")\n",
                "plt.show(fig)\n",
                "plt.close(fig)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Computing FID\")\n",
                "target_mu, target_sigma = get_loader_stats(\n",
                "    Y_test_loader, BATCH_SIZE, FID_EPOCHS, verbose=True, use_Y=False\n",
                ")\n",
                "gen_mu, gen_sigma = get_linked_pushed_loader_stats(\n",
                "    inference_Ts,\n",
                "    X_test_loader,\n",
                "    n_epochs=FID_EPOCHS,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    verbose=True,\n",
                ")\n",
                "fid = calculate_frechet_distance(gen_mu, gen_sigma, target_mu, target_sigma)\n",
                "print(f\"FID={fid}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Computing Accuracy\")\n",
                "accuracy = get_linked_pushed_loader_accuracy(inference_Ts, X_test_loader, classifier)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Computing Metrics\")\n",
                "metrics = get_linked_pushed_loader_metrics(\n",
                "    inference_Ts,\n",
                "    XY_test_sampler.loader,\n",
                "    n_epochs=FID_EPOCHS,\n",
                "    verbose=True,\n",
                "    log_metrics=[\"LPIPS\", \"PSNR\", \"SSIM\", \"MSE\", \"MAE\"],\n",
                ")\n",
                "print(f\"metrics={metrics}\")"
            ]
        }
    ],
    "metadata": {
        "celltoolbar": "Tags",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
