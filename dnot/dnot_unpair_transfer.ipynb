{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Imports\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import gc\n",
                "import json\n",
                "import warnings\n",
                "\n",
                "import torch\n",
                "import torchvision.datasets as datasets\n",
                "import numpy as np\n",
                "from diffusers import DDIMScheduler\n",
                "from tensorboardX import SummaryWriter\n",
                "from torchvision.transforms import Compose, ToTensor, Resize, Normalize\n",
                "\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import PngImagePlugin\n",
                "from tqdm import tqdm_notebook as tqdm\n",
                "from IPython.display import clear_output\n",
                "\n",
                "sys.path.append(\"..\")\n",
                "from src.resnet2 import ResNet_D\n",
                "from src.unet import UNet\n",
                "from src.tools import (\n",
                "    set_random_seed,\n",
                "    unfreeze,\n",
                "    freeze,\n",
                "    weights_init_D,\n",
                "    fig2tensor,\n",
                "    get_all_pivotal,\n",
                "    get_step_t_pivotal,\n",
                "    linked_push,\n",
                ")\n",
                "from src.plotters import (\n",
                "    plot_linked_pushed_images,\n",
                "    plot_linked_pushed_random_class_images,\n",
                ")\n",
                "from src.samplers import (\n",
                "    SubsetGuidedSampler,\n",
                "    SubsetGuidedDataset,\n",
                "    get_indicies_subset,\n",
                ")\n",
                "\n",
                "LARGE_ENOUGH_NUMBER = 100\n",
                "PngImagePlugin.MAX_TEXT_CHUNK = LARGE_ENOUGH_NUMBER * (1024**2)\n",
                "\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "%matplotlib inline "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gc.collect()\n",
                "torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Config\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "SEED = 0x3060\n",
                "set_random_seed(SEED)\n",
                "\n",
                "# dataset choosing\n",
                "DATASET, DATASET_PATH = \"fmnist2mnist\", \"../datasets/\"\n",
                "# DATASET, DATASET_PATH = \"mnist2fmnist\", \"../datasets/\"\n",
                "\n",
                "# DATASET, DATASET_PATH = \"usps2mnist\", \"../datasets/\"\n",
                "# DATASET, DATASET_PATH = \"mnist2usps\", \"../datasets/\"\n",
                "\n",
                "# DATASET, DATASET_PATH = \"usps2fmnist\", \"../datasets/\"\n",
                "# DATASET, DATASET_PATH = \"fmnist2usps\", \"../datasets/\"\n",
                "\n",
                "# DATASET, DATASET_PATH = \"mnistm2mnist\", \"../datasets/\"\n",
                "# DATASET, DATASET_PATH = \"mnist2mnistm\", \"../datasets/\"\n",
                "\n",
                "IMG_SIZE = 32\n",
                "DATASET1_CHANNELS = 1\n",
                "DATASET2_CHANNELS = 1\n",
                "\n",
                "# the step number adding noise in diffusion process\n",
                "DIFFUSION_STEPS = 1000\n",
                "PIVOTAL_LIST = [20, 50, 100]\n",
                "\n",
                "# GPU choosing\n",
                "DEVICE_ID = 1\n",
                "assert torch.cuda.is_available()\n",
                "torch.cuda.set_device(f\"cuda:{DEVICE_ID}\")\n",
                "\n",
                "CONTINUE = [0, 0]  # first is for step, last is for sdes\n",
                "\n",
                "# All hyperparameters below is set to the values used for the experiments, which discribed in the article\n",
                "\n",
                "# training algorithm settings\n",
                "STRATEGY = \"Adapt\"  # 'Fix' or 'Adapt'\n",
                "T_ITERS = 10\n",
                "MAX_STEPS = 5000 + 1\n",
                "\n",
                "# data sample settings\n",
                "BATCH_SIZE = 16\n",
                "SUBSET_SIZE = 4\n",
                "SUBSET_CLASS = 3\n",
                "NUM_LABELED = \"all\"  # \"all\" or int value, sunch as 10\n",
                "\n",
                "# model settings\n",
                "UNET_BASE_FACTOR = 48\n",
                "\n",
                "# optimizer settings\n",
                "D_LR, T_LR = 1e-4, 1e-4\n",
                "T_GRADIENT_MAX_NORM = float(1e5)\n",
                "D_GRADIENT_MAX_NORM = float(1e5)\n",
                "SCHEDULER_MILESTONES = [1500, 3000, 4000]\n",
                "\n",
                "# plot settings\n",
                "GRAY_PLOTS = True\n",
                "PLOT_N_SAMPLES = 8\n",
                "\n",
                "# log settings\n",
                "TRACK_VAR_INTERVAL = 500\n",
                "PLOT_INTERVAL = 500\n",
                "CKPT_INTERVAL = 1000\n",
                "\n",
                "FID_EPOCHS = 1\n",
                "\n",
                "EXP_NAME = f\"DNOT_Unpair_{DATASET}_{SUBSET_CLASS}_{STRATEGY}_{SEED}\"\n",
                "OUTPUT_PATH = f\"../saved_models/{EXP_NAME}/\"\n",
                "\n",
                "if not os.path.exists(OUTPUT_PATH):\n",
                "    os.makedirs(OUTPUT_PATH)\n",
                "\n",
                "writer = SummaryWriter(f\"../logdir/{EXP_NAME}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "config = dict(\n",
                "    SEED=SEED,\n",
                "    DATASET=DATASET,\n",
                "    IMG_SIZE=IMG_SIZE,\n",
                "    DATASET1_CHANNELS=DATASET1_CHANNELS,\n",
                "    DATASET2_CHANNELS=DATASET2_CHANNELS,\n",
                "    DIFFUSION_STEPS=DIFFUSION_STEPS,\n",
                "    PIVOTAL_LIST=PIVOTAL_LIST,\n",
                "    STRATEGY=STRATEGY,\n",
                "    T_ITERS=T_ITERS,\n",
                "    MAX_STEPS=MAX_STEPS,\n",
                "    UNET_BASE_FACTOR=UNET_BASE_FACTOR,\n",
                "    BATCH_SIZE=BATCH_SIZE,\n",
                "    SUBSET_SIZE=SUBSET_SIZE,\n",
                "    SUB_CLASS=SUBSET_CLASS,\n",
                "    D_LR=D_LR,\n",
                "    T_LR=T_LR,\n",
                "    T_GRADIENT_MAX_NORM=T_GRADIENT_MAX_NORM,\n",
                "    D_GRADIENT_MAX_NORM=D_GRADIENT_MAX_NORM,\n",
                "    SCHEDULER_MILESTONES=SCHEDULER_MILESTONES,\n",
                "    GRAY_PLOTS=GRAY_PLOTS,\n",
                "    PLOT_N_SAMPLES=PLOT_N_SAMPLES,\n",
                "    TRACK_VAR_INTERVAL=TRACK_VAR_INTERVAL,\n",
                "    PLOT_INTERVAL=PLOT_INTERVAL,\n",
                "    CKPT_INTERVAL=CKPT_INTERVAL,\n",
                "    FID_EPOCHS=FID_EPOCHS,\n",
                ")\n",
                "with open(os.path.join(OUTPUT_PATH, \"config.json\"), \"w\") as json_file:\n",
                "    json_str = json.dumps(config, indent=4)\n",
                "    json_file.write(json_str)\n",
                "\n",
                "log = dict(CONTINUE=CONTINUE)\n",
                "with open(os.path.join(OUTPUT_PATH, \"log.json\"), \"w\") as log_file:\n",
                "    log_str = json.dumps(log, indent=4)\n",
                "    log_file.write(log_str)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Initialize samplers\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "###  data sampler"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "source_subset = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
                "new_labels_source = {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}\n",
                "target_subset = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
                "new_labels_target = {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}\n",
                "\n",
                "SUBSET_WEIGHT = [0 for _ in range(len(source_subset))]\n",
                "SUBSET_WEIGHT[SUBSET_CLASS] = 1.0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "source_transform = Compose(\n",
                "    [\n",
                "        Resize((IMG_SIZE, IMG_SIZE)),\n",
                "        ToTensor(),\n",
                "        Normalize((0.5), (0.5)),\n",
                "    ]\n",
                ")\n",
                "target_transform = source_transform\n",
                "\n",
                "if DATASET == \"fmnist2mnist\":\n",
                "    source = datasets.FashionMNIST\n",
                "    target = datasets.MNIST\n",
                "\n",
                "elif DATASET == \"mnist2fmnist\":\n",
                "    source = datasets.MNIST\n",
                "    target = datasets.FashionMNIST\n",
                "\n",
                "elif DATASET == \"mnist2usps\":\n",
                "    source = datasets.MNIST\n",
                "    target = datasets.USPS\n",
                "\n",
                "elif DATASET == \"usps2mnist\":\n",
                "    source = datasets.USPS\n",
                "    target = datasets.MNIST\n",
                "\n",
                "elif DATASET == \"usps2fmnist\":\n",
                "    source = datasets.USPS\n",
                "    target = datasets.FashionMNIST\n",
                "\n",
                "elif DATASET == \"fmnist2usps\":\n",
                "    source = datasets.FashionMNIST\n",
                "    target = datasets.USPS\n",
                "else:\n",
                "    raise Exception(f\"{DATASET} not support now...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "source_train = source(\n",
                "    root=DATASET_PATH, train=True, download=True, transform=source_transform\n",
                ")\n",
                "subset_samples, labels, source_class_indicies = get_indicies_subset(\n",
                "    source_train,\n",
                "    new_labels=new_labels_source,\n",
                "    classes=len(source_subset),\n",
                "    subset_classes=source_subset,\n",
                ")\n",
                "source_train = torch.utils.data.TensorDataset(\n",
                "    torch.stack(subset_samples), torch.LongTensor(labels)\n",
                ")\n",
                "\n",
                "\n",
                "target_train = target(\n",
                "    root=DATASET_PATH, train=True, download=True, transform=target_transform\n",
                ")\n",
                "target_subset_samples, target_labels, target_class_indicies = get_indicies_subset(\n",
                "    target_train,\n",
                "    new_labels=new_labels_target,\n",
                "    classes=len(target_subset),\n",
                "    subset_classes=target_subset,\n",
                ")\n",
                "target_train = torch.utils.data.TensorDataset(\n",
                "    torch.stack(target_subset_samples), torch.LongTensor(target_labels)\n",
                ")\n",
                "\n",
                "train_set = SubsetGuidedDataset(\n",
                "    source_train,\n",
                "    target_train,\n",
                "    num_labeled=NUM_LABELED,\n",
                "    in_indicies=source_class_indicies,\n",
                "    out_indicies=target_class_indicies,\n",
                ")\n",
                "\n",
                "full_set = SubsetGuidedDataset(\n",
                "    source_train,\n",
                "    target_train,\n",
                "    num_labeled=\"all\",\n",
                "    in_indicies=source_class_indicies,\n",
                "    out_indicies=target_class_indicies,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "T_XY_sampler = SubsetGuidedSampler(\n",
                "    train_set, subsetsize=SUBSET_SIZE, weight=SUBSET_WEIGHT\n",
                ")\n",
                "D_XY_sampler = SubsetGuidedSampler(full_set, subsetsize=1, weight=SUBSET_WEIGHT)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "torch.cuda.empty_cache()\n",
                "gc.collect()\n",
                "clear_output()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### pivotal sampler\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "SCHEDULER = DDIMScheduler(num_train_timesteps=DIFFUSION_STEPS)\n",
                "\n",
                "\n",
                "def sample_all_pivotal(\n",
                "    XY_sampler: SubsetGuidedSampler,\n",
                "    batch_size: int = 4,\n",
                ") -> list[torch.Tensor]:\n",
                "    source, target = XY_sampler.sample(batch_size)\n",
                "\n",
                "    return get_all_pivotal(\n",
                "        source,\n",
                "        target,\n",
                "        SCHEDULER,\n",
                "        PIVOTAL_LIST,\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_all_pivotal(\n",
                "    source: torch.Tensor,\n",
                "    target: torch.Tensor,\n",
                "    gray: bool = False,\n",
                "):\n",
                "    pivotal_path = get_all_pivotal(source, target, SCHEDULER, PIVOTAL_LIST)\n",
                "\n",
                "    imgs: np.ndarray = (\n",
                "        torch.stack(pivotal_path)\n",
                "        .to(\"cpu\")\n",
                "        .permute(0, 2, 3, 1)\n",
                "        .mul(0.5)\n",
                "        .add(0.5)\n",
                "        .numpy()\n",
                "        .clip(0, 1)\n",
                "    )\n",
                "    nrows, ncols = 1, len(pivotal_path)\n",
                "    fig = plt.figure(figsize=(1.5 * ncols, 1.5 * nrows), dpi=150)\n",
                "    for i, img in enumerate(imgs):\n",
                "        ax = fig.add_subplot(nrows, ncols, i + 1)\n",
                "        if gray:\n",
                "            ax.imshow(img, cmap=\"gray\")\n",
                "        else:\n",
                "            ax.imshow(img)\n",
                "        ax.get_yaxis().set_visible(False)\n",
                "        ax.get_xaxis().set_visible(False)\n",
                "        ax.set_yticks([])\n",
                "        ax.set_xticks([])\n",
                "        ax.set_title(f\"$X_{i}$\", fontsize=24)\n",
                "        if i == imgs.shape[0] - 1:\n",
                "            ax.set_title(\"Y\", fontsize=24)\n",
                "\n",
                "    torch.cuda.empty_cache()\n",
                "    gc.collect()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Initialize models\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### init models\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "Ts, Ds = [], []\n",
                "T_OPTs, D_OPTs = [], []\n",
                "T_SCHEDULERs, D_SCHEDULERs = [], []\n",
                "\n",
                "for i in range(len(PIVOTAL_LIST) * 2):\n",
                "    T = UNet(DATASET1_CHANNELS, DATASET2_CHANNELS, base_factor=UNET_BASE_FACTOR).cuda()\n",
                "    Ts.append(T)\n",
                "\n",
                "    D = ResNet_D(IMG_SIZE, nc=DATASET2_CHANNELS).cuda()\n",
                "    D.apply(weights_init_D)\n",
                "    Ds.append(D)\n",
                "\n",
                "    T_opt = torch.optim.Adam(T.parameters(), lr=T_LR, weight_decay=1e-10)\n",
                "    D_opt = torch.optim.Adam(D.parameters(), lr=D_LR, weight_decay=1e-10)\n",
                "    T_OPTs.append(T_opt)\n",
                "    D_OPTs.append(D_opt)\n",
                "\n",
                "    T_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
                "        T_opt, milestones=SCHEDULER_MILESTONES, gamma=0.5\n",
                "    )\n",
                "    D_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
                "        D_opt, milestones=SCHEDULER_MILESTONES, gamma=0.5\n",
                "    )\n",
                "    T_SCHEDULERs.append(T_scheduler)\n",
                "    D_SCHEDULERs.append(D_scheduler)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Load weights for continue training\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if CONTINUE[0] > 0 or CONTINUE[1] > 0:\n",
                "    print(\"Loading weights for continue training\")\n",
                "    if STRATEGY == \"Adapt\":\n",
                "        for i, (T, T_opt, T_scheduler, D, D_opt, D_scheduler) in enumerate(\n",
                "            zip(\n",
                "                Ts,\n",
                "                T_OPTs,\n",
                "                T_SCHEDULERs,\n",
                "                Ds,\n",
                "                D_OPTs,\n",
                "                D_SCHEDULERs,\n",
                "            )\n",
                "        ):\n",
                "            if i > CONTINUE[1]:\n",
                "                continue\n",
                "            if i < CONTINUE[1]:\n",
                "                CKPT_DIR = os.path.join(OUTPUT_PATH, f\"iter{MAX_STEPS - 1}/\")\n",
                "            if i == CONTINUE[1]:\n",
                "                CKPT_DIR = os.path.join(OUTPUT_PATH, f\"iter{CONTINUE[0] - 1}/\")\n",
                "\n",
                "            T.load_state_dict(torch.load(os.path.join(CKPT_DIR, f\"T{i}_{SEED}.pt\")))\n",
                "            print(f\"{CKPT_DIR} T{i}_{SEED}.pt, loaded\")\n",
                "\n",
                "            T_opt.load_state_dict(\n",
                "                torch.load(os.path.join(CKPT_DIR, f\"T_opt{i}_{SEED}.pt\"))\n",
                "            )\n",
                "            print(f\"{CKPT_DIR} T_opt{i}_{SEED}.pt, loaded\")\n",
                "\n",
                "            T_scheduler.load_state_dict(\n",
                "                torch.load(os.path.join(CKPT_DIR, f\"T_scheduler{i}_{SEED}.pt\"))\n",
                "            )\n",
                "            print(f\"{CKPT_DIR} T_scheduler{i}_{SEED}.pt, loaded\")\n",
                "\n",
                "            D.load_state_dict(torch.load(os.path.join(CKPT_DIR, f\"D{i}_{SEED}.pt\")))\n",
                "            print(f\"{CKPT_DIR} D{i}_{SEED}.pt, loaded\")\n",
                "\n",
                "            D_opt.load_state_dict(\n",
                "                torch.load(os.path.join(CKPT_DIR, f\"D_opt{i}_{SEED}.pt\"))\n",
                "            )\n",
                "            print(f\"{CKPT_DIR} D_opt{i}_{SEED}.pt, loaded\")\n",
                "\n",
                "            D_scheduler.load_state_dict(\n",
                "                torch.load(os.path.join(CKPT_DIR, f\"D_scheduler{i}_{SEED}.pt\"))\n",
                "            )\n",
                "            print(f\"{CKPT_DIR} D_scheduler{i}_{SEED}.pt, loaded\")\n",
                "\n",
                "    if STRATEGY == \"Fix\":\n",
                "        CKPT_DIR = os.path.join(OUTPUT_PATH, f\"iter{CONTINUE[0] - 1}/\")\n",
                "        for i, (T, T_opt, T_scheduler, D, D_opt, D_scheduler) in enumerate(\n",
                "            zip(\n",
                "                Ts,\n",
                "                T_OPTs,\n",
                "                T_SCHEDULERs,\n",
                "                Ds,\n",
                "                D_OPTs,\n",
                "                D_SCHEDULERs,\n",
                "            )\n",
                "        ):\n",
                "            T.load_state_dict(torch.load(os.path.join(CKPT_DIR, f\"T{i}_{SEED}.pt\")))\n",
                "            print(f\"{CKPT_DIR} T{i}_{SEED}.pt, loaded\")\n",
                "\n",
                "            T_opt.load_state_dict(\n",
                "                torch.load(os.path.join(CKPT_DIR, f\"T_opt{i}_{SEED}.pt\"))\n",
                "            )\n",
                "            print(f\"{CKPT_DIR} T_opt{i}_{SEED}.pt, loaded\")\n",
                "\n",
                "            T_scheduler.load_state_dict(\n",
                "                torch.load(os.path.join(CKPT_DIR, f\"T_scheduler{i}_{SEED}.pt\"))\n",
                "            )\n",
                "            print(f\"{CKPT_DIR} T_scheduler{i}_{SEED}.pt, loaded\")\n",
                "\n",
                "            D.load_state_dict(torch.load(os.path.join(CKPT_DIR, f\"D{i}_{SEED}.pt\")))\n",
                "            print(f\"{CKPT_DIR} D{i}_{SEED}.pt, loaded\")\n",
                "\n",
                "            D_opt.load_state_dict(\n",
                "                torch.load(os.path.join(CKPT_DIR, f\"D_opt{i}_{SEED}.pt\"))\n",
                "            )\n",
                "            print(f\"{CKPT_DIR} D_opt{i}_{SEED}.pt, loaded\")\n",
                "\n",
                "            D_scheduler.load_state_dict(\n",
                "                torch.load(os.path.join(CKPT_DIR, f\"D_scheduler{i}_{SEED}.pt\"))\n",
                "            )\n",
                "            print(f\"{CKPT_DIR} D_scheduler{i}_{SEED}.pt, loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Plots Test\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_test_fixed, Y_test_fixed = D_XY_sampler.sample(PLOT_N_SAMPLES)\n",
                "X_test_fixed, Y_test_fixed = (\n",
                "    X_test_fixed.flatten(0, 1),\n",
                "    Y_test_fixed.flatten(0, 1),\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_all_pivotal(\n",
                "    X_test_fixed[0],\n",
                "    Y_test_fixed[0],\n",
                "    gray=GRAY_PLOTS,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plot_linked_pushed_images(\n",
                "    X_test_fixed,\n",
                "    Y_test_fixed,\n",
                "    Ts,\n",
                "    gray=GRAY_PLOTS,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plot_linked_pushed_random_class_images(\n",
                "    D_XY_sampler,\n",
                "    Ts,\n",
                "    plot_n_samples=PLOT_N_SAMPLES,\n",
                "    gray=GRAY_PLOTS,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Train\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gc.collect()\n",
                "torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_cat_pivotal_tr(XY_sampler1, XY_sampler2, batch_size):\n",
                "    X_tr = sample_all_pivotal(XY_sampler1, batch_size)\n",
                "    Y_tr = sample_all_pivotal(XY_sampler2, batch_size)\n",
                "    assert len(X_tr) == len(Y_tr)\n",
                "\n",
                "    tr = []\n",
                "    length = len(X_tr)\n",
                "    mid_indx = length // 2\n",
                "    tr.extend(X_tr[: mid_indx + 1])\n",
                "    tr.extend(Y_tr[mid_indx + 1 :])\n",
                "\n",
                "    del X_tr[mid_indx + 1 :]\n",
                "    del Y_tr[: mid_indx + 1]\n",
                "\n",
                "    gc.collect()\n",
                "    torch.cuda.empty_cache()\n",
                "    return tr"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Fix strategy\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if STRATEGY == \"Fix\":\n",
                "    pbar = tqdm(total=MAX_STEPS, initial=max(0, CONTINUE[0]), leave=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if STRATEGY == \"Fix\":\n",
                "    for step in range(max(0, CONTINUE[0]), MAX_STEPS):\n",
                "        for k in range(T_ITERS):\n",
                "            # === sample path ===\n",
                "            pivotal_path = sample_all_pivotal(T_XY_sampler, BATCH_SIZE)\n",
                "            for i, (T, T_opt, D) in enumerate(zip(Ts, T_OPTs, Ds)):\n",
                "                freeze(D)\n",
                "                unfreeze(T)\n",
                "                # === sample input ===\n",
                "                X, Y = pivotal_path[i], pivotal_path[i + 1]\n",
                "                # === clear grad ===\n",
                "                T_opt.zero_grad()\n",
                "                # === forward ===\n",
                "                T_X = (\n",
                "                    T(X.flatten(0, 1))\n",
                "                    .permute(1, 2, 3, 0)\n",
                "                    .reshape(DATASET2_CHANNELS, IMG_SIZE, IMG_SIZE, -1, SUBSET_SIZE)\n",
                "                    .permute(3, 4, 0, 1, 2)\n",
                "                )\n",
                "                # === loss ===\n",
                "                T_var = (\n",
                "                    0.5\n",
                "                    * torch.cdist(\n",
                "                        T_X.flatten(start_dim=2), T_X.flatten(start_dim=2)\n",
                "                    ).mean()\n",
                "                    * SUBSET_SIZE\n",
                "                    / (SUBSET_SIZE - 1)\n",
                "                )\n",
                "                cost = (Y - T_X).flatten(start_dim=2).norm(dim=2).mean()\n",
                "                T_loss = cost - T_var - D(T_X.flatten(start_dim=0, end_dim=1)).mean()\n",
                "                writer.add_scalar(f\"T_loss/T{i}\", T_loss.item(), step)\n",
                "                # === backward ===\n",
                "                T_loss.backward()\n",
                "                # === clip grad ===\n",
                "                T_gradient_norm = torch.nn.utils.clip_grad_norm_(\n",
                "                    T.parameters(), max_norm=T_GRADIENT_MAX_NORM\n",
                "                )\n",
                "                # === optim ===\n",
                "                T_opt.step()\n",
                "            # # === clear tmp variables ===\n",
                "            # del pivotal_path, X, Y, T_X, T_loss\n",
                "            # gc.collect()\n",
                "            # torch.cuda.empty_cache()\n",
                "        # === update lr ===\n",
                "        for T_scheduler in T_SCHEDULERs:\n",
                "            T_scheduler.step()\n",
                "\n",
                "        for T, D in zip(Ts, Ds):\n",
                "            freeze(T)\n",
                "            freeze(D)\n",
                "        # === sample path ===\n",
                "        # two way\n",
                "        #   1. concatnat path: pivotal_path = get_cat_pivotal_tr(T_XY_sampler, D_XY_sampler, BATCH_SIZE)\n",
                "        #   2. just 1 path: pivotal_path = sample_all_pivotal(D_XY_sampler, BATCH_SIZE)\n",
                "        pivotal_path = sample_all_pivotal(D_XY_sampler, BATCH_SIZE)\n",
                "        for i, (D, D_opt, D_scheduler, T) in enumerate(\n",
                "            zip(Ds, D_OPTs, D_SCHEDULERs, Ts)\n",
                "        ):\n",
                "            freeze(T)\n",
                "            unfreeze(D)\n",
                "            # === sample input ===\n",
                "            X, Y = pivotal_path[i], pivotal_path[i + 1]\n",
                "            # === clear grad ===\n",
                "            D_opt.zero_grad()\n",
                "            # === forward ===\n",
                "            with torch.no_grad():\n",
                "                T_X = T(X.flatten(start_dim=0, end_dim=1))\n",
                "            # === loss ===\n",
                "            D_loss = D(T_X).mean() - D(Y.flatten(start_dim=0, end_dim=1)).mean()\n",
                "            writer.add_scalar(f\"D_loss/D{i}\", D_loss.item(), step)\n",
                "            # === backward ===\n",
                "            D_loss.backward()\n",
                "            # === clip grad ===\n",
                "            D_gradient_norm = torch.nn.utils.clip_grad_norm_(\n",
                "                D.parameters(), max_norm=D_GRADIENT_MAX_NORM\n",
                "            )\n",
                "            # === optim ===\n",
                "            D_opt.step()\n",
                "            # # === clear tmp variables ===\n",
                "            # del pivotal_path, X, Y, T_X, D_loss\n",
                "            # gc.collect()\n",
                "            # torch.cuda.empty_cache()\n",
                "            # === update lr ===\n",
                "            D_scheduler.step()\n",
                "\n",
                "        CONTINUE[0] += 1\n",
                "        pbar.update(1)\n",
                "\n",
                "        if step % PLOT_INTERVAL == 0:\n",
                "            clear_output(wait=True)\n",
                "            print(f\"{step = }, Plotting\")\n",
                "\n",
                "            print(\"Fix Test Images\")\n",
                "            fig, axes = plot_linked_pushed_images(\n",
                "                X_test_fixed,\n",
                "                Y_test_fixed,\n",
                "                Ts,\n",
                "                gray=GRAY_PLOTS,\n",
                "            )\n",
                "            writer.add_image(\"Fix Test Images\", fig2tensor(fig), step)\n",
                "            plt.show(fig)\n",
                "            plt.close(fig)\n",
                "            print(\"Random Test Images\")\n",
                "            fig, axes = plot_linked_pushed_random_class_images(\n",
                "                D_XY_sampler,\n",
                "                Ts,\n",
                "                plot_n_samples=PLOT_N_SAMPLES,\n",
                "                gray=GRAY_PLOTS,\n",
                "            )\n",
                "            writer.add_image(\"Random Test Images\", fig2tensor(fig), step)\n",
                "            plt.show(fig)\n",
                "            plt.close(fig)\n",
                "\n",
                "        if step != 0 and step % CKPT_INTERVAL == 0:\n",
                "            CKPT_DIR = os.path.join(OUTPUT_PATH, f\"iter{step}/\")\n",
                "            os.makedirs(CKPT_DIR, exist_ok=True)\n",
                "            for i, (T, T_opt, T_scheduler, D, D_opt, D_scheduler) in enumerate(\n",
                "                zip(\n",
                "                    Ts,\n",
                "                    T_OPTs,\n",
                "                    T_SCHEDULERs,\n",
                "                    Ds,\n",
                "                    D_OPTs,\n",
                "                    D_SCHEDULERs,\n",
                "                )\n",
                "            ):\n",
                "                torch.save(T.state_dict(), os.path.join(CKPT_DIR, f\"T{i}_{SEED}.pt\"))\n",
                "                torch.save(D.state_dict(), os.path.join(CKPT_DIR, f\"D{i}_{SEED}.pt\"))\n",
                "\n",
                "                torch.save(\n",
                "                    D_opt.state_dict(),\n",
                "                    os.path.join(CKPT_DIR, f\"D_opt{i}_{SEED}.pt\"),\n",
                "                )\n",
                "                torch.save(\n",
                "                    T_opt.state_dict(),\n",
                "                    os.path.join(CKPT_DIR, f\"T_opt{i}_{SEED}.pt\"),\n",
                "                )\n",
                "                torch.save(\n",
                "                    D_scheduler.state_dict(),\n",
                "                    os.path.join(CKPT_DIR, f\"D_scheduler{i}_{SEED}.pt\"),\n",
                "                )\n",
                "                torch.save(\n",
                "                    T_scheduler.state_dict(),\n",
                "                    os.path.join(CKPT_DIR, f\"T_scheduler{i}_{SEED}.pt\"),\n",
                "                )\n",
                "\n",
                "            log[\"CONTINUE\"] = CONTINUE\n",
                "            with open(os.path.join(OUTPUT_PATH, \"log.json\"), \"w\") as log_file:\n",
                "                log_str = json.dumps(log, indent=4)\n",
                "                log_file.write(log_str)\n",
                "\n",
                "        if step % TRACK_VAR_INTERVAL == 0:\n",
                "            # after training, using test_transport.ipynb to get fid acc ...\n",
                "            pass\n",
                "\n",
                "    # gc.collect()\n",
                "    # torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Adapt strategy\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if STRATEGY == \"Adapt\":\n",
                "    spbar = tqdm(total=len(Ts), initial=0, position=0, desc=\"total\")\n",
                "    pbar = tqdm(\n",
                "        total=MAX_STEPS,\n",
                "        initial=max(0, CONTINUE[0]),\n",
                "        position=1,\n",
                "        desc=\"single\",\n",
                "        leave=True,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if STRATEGY == \"Adapt\":\n",
                "    for i, (T, T_opt, T_scheduler, D, D_opt, D_scheduler) in enumerate(\n",
                "        zip(\n",
                "            Ts,\n",
                "            T_OPTs,\n",
                "            T_SCHEDULERs,\n",
                "            Ds,\n",
                "            D_OPTs,\n",
                "            D_SCHEDULERs,\n",
                "        )\n",
                "    ):\n",
                "        if i < CONTINUE[1]:\n",
                "            spbar.update(1)\n",
                "            continue\n",
                "\n",
                "        for _T, _D in zip(Ts, Ds):\n",
                "            freeze(_T)\n",
                "            freeze(_D)\n",
                "\n",
                "        for step in range(max(0, CONTINUE[0]), MAX_STEPS):\n",
                "            for k in range(T_ITERS):\n",
                "                freeze(D)\n",
                "                unfreeze(T)\n",
                "\n",
                "                # === sampler path ===\n",
                "                pivotal_path = sample_all_pivotal(T_XY_sampler, BATCH_SIZE)\n",
                "                # === sample input ===\n",
                "                X, Y = pivotal_path[0], pivotal_path[i + 1]\n",
                "                X = linked_push(\n",
                "                    Ts[:i],\n",
                "                    X.flatten(0, 1),\n",
                "                    \"T_X\",\n",
                "                )\n",
                "                # X.requires_grad_(True)\n",
                "                # Y.requires_grad_(False)\n",
                "\n",
                "                # === clear grad ===\n",
                "                T_opt.zero_grad()\n",
                "                # === forward ===\n",
                "                T_X = (\n",
                "                    T(X)\n",
                "                    .permute(1, 2, 3, 0)\n",
                "                    .reshape(DATASET2_CHANNELS, IMG_SIZE, IMG_SIZE, -1, SUBSET_SIZE)\n",
                "                    .permute(3, 4, 0, 1, 2)\n",
                "                )\n",
                "                # === loss ===\n",
                "                T_var = (\n",
                "                    0.5\n",
                "                    * torch.cdist(\n",
                "                        T_X.flatten(start_dim=2), T_X.flatten(start_dim=2)\n",
                "                    ).mean()\n",
                "                    * SUBSET_SIZE\n",
                "                    / (SUBSET_SIZE - 1)\n",
                "                )\n",
                "                cost = (Y - T_X).flatten(start_dim=2).norm(dim=2).mean()\n",
                "                T_loss = cost - T_var - D(T_X.flatten(start_dim=0, end_dim=1)).mean()\n",
                "                writer.add_scalar(f\"T_loss/T{i}\", T_loss.item(), step)\n",
                "                # === backward ===\n",
                "                T_loss.backward()\n",
                "                # === clip grad ===\n",
                "                T_gradient_norm = torch.nn.utils.clip_grad_norm_(\n",
                "                    T.parameters(), max_norm=T_GRADIENT_MAX_NORM\n",
                "                )\n",
                "                # === optim ===\n",
                "                T_opt.step()\n",
                "                # # === clear tmp variables ===\n",
                "                # del pivotal_path, X, Y, T_X, T_loss\n",
                "                # gc.collect()\n",
                "                # torch.cuda.empty_cache()\n",
                "            # === update lr ===\n",
                "            T_scheduler.step()\n",
                "\n",
                "            freeze(T)\n",
                "            unfreeze(D)\n",
                "            # === sampler path ===\n",
                "            # two way\n",
                "            #   1. concatnat path: pivotal_path = get_cat_pivotal_tr(XY_sampler, XY_sampler, BATCH_SIZE)\n",
                "            #   2. just 1 path: pivotal_path = sample_all_pivotal(XY_sampler, BATCH_SIZE)\n",
                "            pivotal_path = sample_all_pivotal(D_XY_sampler, BATCH_SIZE)\n",
                "            # === sample input ===\n",
                "            X, Y = pivotal_path[0], pivotal_path[i + 1]\n",
                "            X = linked_push(\n",
                "                Ts[:i],\n",
                "                X.flatten(0, 1),\n",
                "                \"T_X\",\n",
                "            )\n",
                "            # X.requires_grad_(True)\n",
                "            # Y.requires_grad_(False)\n",
                "\n",
                "            # === clear grad ===\n",
                "            D_opt.zero_grad()\n",
                "            # === forward ===\n",
                "            T_X = T(X)\n",
                "            # === loss ===\n",
                "            D_loss = D(T_X).mean() - D(Y.flatten(start_dim=0, end_dim=1)).mean()\n",
                "            writer.add_scalar(f\"D_loss/D{i}\", D_loss.item(), step)\n",
                "            # === backward ===\n",
                "            D_loss.backward()\n",
                "            # === clip grad ===\n",
                "            D_gradient_norm = torch.nn.utils.clip_grad_norm_(\n",
                "                D.parameters(), max_norm=D_GRADIENT_MAX_NORM\n",
                "            )\n",
                "            # === optim ===\n",
                "            D_opt.step()\n",
                "            # # === clear tmp variables ===\n",
                "            # del pivotal_path, X, Y, T_X, D_loss\n",
                "            # gc.collect()\n",
                "            # torch.cuda.empty_cache()\n",
                "            # === update lr ===\n",
                "            D_scheduler.step()\n",
                "\n",
                "            CONTINUE[0] += 1\n",
                "            pbar.update(1)\n",
                "\n",
                "            if step % PLOT_INTERVAL == 0:\n",
                "                clear_output(wait=True)\n",
                "                print(f\"training {i}-th T, {step = }, Plotting\")\n",
                "\n",
                "                print(\"Fixed Test Images\")\n",
                "                fig, axes = plot_linked_pushed_images(\n",
                "                    X_test_fixed,\n",
                "                    Y_test_fixed,\n",
                "                    Ts,\n",
                "                    gray=GRAY_PLOTS,\n",
                "                )\n",
                "                writer.add_image(\n",
                "                    f\"Fix Test Images/T[0..={i}]\",\n",
                "                    fig2tensor(fig),\n",
                "                    step,\n",
                "                )\n",
                "                plt.show(fig)\n",
                "                plt.close(fig)\n",
                "\n",
                "                print(\"Random Test Images\")\n",
                "                fig, axes = plot_linked_pushed_random_class_images(\n",
                "                    D_XY_sampler,\n",
                "                    Ts,\n",
                "                    plot_n_samples=PLOT_N_SAMPLES,\n",
                "                    gray=GRAY_PLOTS,\n",
                "                )\n",
                "                writer.add_image(\n",
                "                    f\"Random Test Images/T[0..={i}]\",\n",
                "                    fig2tensor(fig),\n",
                "                    step,\n",
                "                )\n",
                "                plt.show(fig)\n",
                "                plt.close(fig)\n",
                "\n",
                "            if step != 0 and step % CKPT_INTERVAL == 0:\n",
                "                CKPT_DIR = os.path.join(OUTPUT_PATH, f\"iter{step}/\")\n",
                "                os.makedirs(CKPT_DIR, exist_ok=True)\n",
                "                torch.save(T.state_dict(), os.path.join(CKPT_DIR, f\"T{i}_{SEED}.pt\"))\n",
                "                torch.save(D.state_dict(), os.path.join(CKPT_DIR, f\"D{i}_{SEED}.pt\"))\n",
                "\n",
                "                torch.save(\n",
                "                    D_opt.state_dict(),\n",
                "                    os.path.join(CKPT_DIR, f\"D_opt{i}_{SEED}.pt\"),\n",
                "                )\n",
                "                torch.save(\n",
                "                    T_opt.state_dict(),\n",
                "                    os.path.join(CKPT_DIR, f\"T_opt{i}_{SEED}.pt\"),\n",
                "                )\n",
                "                torch.save(\n",
                "                    D_scheduler.state_dict(),\n",
                "                    os.path.join(CKPT_DIR, f\"D_scheduler{i}_{SEED}.pt\"),\n",
                "                )\n",
                "                torch.save(\n",
                "                    T_scheduler.state_dict(),\n",
                "                    os.path.join(CKPT_DIR, f\"T_scheduler{i}_{SEED}.pt\"),\n",
                "                )\n",
                "                log = dict(CONTINUE=CONTINUE)\n",
                "                with open(os.path.join(OUTPUT_PATH, \"log.json\"), \"w\") as log_file:\n",
                "                    log_str = json.dumps(log, indent=4)\n",
                "                    log_file.write(log_str)\n",
                "\n",
                "            if i == len(Ts) - 1 and step % TRACK_VAR_INTERVAL == 0:\n",
                "                # after training, using test_transport.ipynb to get fid acc ...\n",
                "                pass\n",
                "\n",
                "            gc.collect()\n",
                "            torch.cuda.empty_cache()\n",
                "\n",
                "        CONTINUE[0] = 0  # reset training steps to 0\n",
                "        pbar.reset()\n",
                "        CONTINUE[1] += 1\n",
                "        spbar.update(1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Clear resources\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    writer.close()\n",
                "    pbar.close()\n",
                "    spbar.close()\n",
                "except Exception as e:\n",
                "    print(e)"
            ]
        }
    ],
    "metadata": {
        "celltoolbar": "Tags",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
